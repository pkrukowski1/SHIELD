import torch
import torch.nn.functional as F

from typing import Tuple, Callable, Union

class IntervalLinear:
    """
    Interval version of a linear layer. Parameters are generated by a hypernetwork,
    so they are not initialized here.

    For description of the attributes please see the docs of
    https://pytorch.org/docs/stable/generated/torch.nn.Linear.html
    """

    def __init__(self, in_features: int, out_features: int, device=None) -> None:
        
        self.in_features = in_features
        self.out_features = out_features
        self.device = device

    def forward(self, mu: torch.Tensor, 
                    eps: torch.Tensor,
                    weight: torch.Tensor,
                    bias: torch.Tensor = None,
                    device: Union[str, torch.device] = "cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            mu (torch.Tensor): Midpoint of the interval.
            eps (torch.Tensor): Radii of the interval.
            weight (torch.Tensor): Matrix of a linear transformation.
            bias (torch.Tensor, optional): Bias of a linear transformation. Defaults to None.
           device (torch.device, optional): Device for calculations, either "cpu" or "cuda".

        Returns:
            tuple: A tuple containing:
                - new_mu (torch.Tensor): 'mu' after the linear transformation.
                - new_eps (torch.Tensor): 'eps' after the linear transformation.
        """

        # Send tensors into devices
        mu     = mu.to(device)
        eps    = eps.to(device)
        weight = weight.to(device)
        bias   = bias.to(device) if bias is not None else bias
        
        # Perform linear transformations
        new_mu = F.linear(
            input=mu,
            weight=weight,
            bias=bias
        )

        new_eps = F.linear(
            input=eps,
            weight=weight.abs(),
            bias=None
        )

        return new_mu, new_eps
    
class IntervalReLU:
    def __init__(self):
        """Initialize IntervalReLU layer."""
        pass
    
    def forward(self, mu: torch.Tensor, eps: torch.Tensor, 
                device: Union[str, torch.device] = "cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """Apply ReLU activation to interval-valued inputs.

        Args:
            mu (torch.Tensor): Midpoint of the interval.
            eps (torch.Tensor): Radii of the interval.
            device (torch.device, optional): Device for calculations, either "cpu" or "cuda". Defaults to "cuda".

        Returns:
            tuple: A tuple containing:
                - new_mu (torch.Tensor): Midpoint after ReLU transformation.
                - new_eps (torch.Tensor): Radii after ReLU transformation.
        """
        mu = mu.to(device)
        eps = eps.to(device)

        z_l, z_u = F.relu(mu-eps), F.relu(mu+eps)
        mu, eps = (z_l+z_u)/2.0, (z_u-z_l)/2.0

        return mu, eps

    
class IntervalConv2d:
    """
    For description of the attributes please see the docs of
    https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0,
                 dilation = 1, groups = 1, padding_mode = 'zeros', device="cuda") -> None:

        
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.groups = groups
        self.padding_mode = padding_mode
        self.device = device
        
    def forward(self, mu: torch.Tensor, 
                eps: torch.Tensor,
                weight: torch.Tensor,
                bias: torch.Tensor,
                device: Union[str, torch.device] = "cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            mu (torch.Tensor): Midpoint of the interval.
            eps (torch.Tensor): Radii of the interval.
            weight (torch.Tensor): Kernels of a convolutional transformation.
            bias (torch.Tensor, optional): Bias of a convolutional transformation. Defaults to None.
           device (torch.device, optional): Device for calculations, either "cpu" or "cuda".

        Returns:
            tuple: A tuple containing:
                - new_mu (torch.Tensor): 'mu' after the convolutional transformation.
                - new_eps (torch.Tensor): 'eps' after the convolutional transformation.
        """

        # Send tensors into devices
        mu     = mu.to(device)
        eps    = eps.to(device)
        weight = weight.to(device)
        bias   = bias.to(device) if bias is not None else bias
        
        # Perform convolutional transformations
        new_mu = F.conv2d(
              input=mu,
              weight=weight,
              bias=bias,
              stride=self.stride,
              padding=self.padding,
              dilation=self.dilation,
              groups=self.groups
          )

        new_eps = F.conv2d(
              input=eps,
              weight=weight.abs(),
              bias=None,
              stride=self.stride,
              padding=self.padding,
              dilation=self.dilation,
              groups=self.groups
          )
        
        return new_mu, new_eps
    
class IntervalFlatten:
    """Interval flattening layer.

    For detailed argument descriptions, see: https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html
    """

    def __init__(self, start_dim: int = 1, end_dim: int = -1) -> None:
        """Initialize IntervalFlatten layer.

        Args:
            start_dim (int, optional): The first dimension to flatten. Defaults to 1.
            end_dim (int, optional): The last dimension to flatten. Defaults to -1.
        """
        self.start_dim = start_dim
        self.end_dim = end_dim

    def forward(self, mu: torch.Tensor, eps: torch.Tensor,
                device: Union[str, torch.device] = "cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """Apply interval flattening.

        Args:
            mu (torch.Tensor): Midpoint of the interval.
            eps (torch.Tensor): Radii of the interval.
            device (torch.device, optional): Device for calculations, either "cpu" or "cuda". Defaults to "cuda".

        Returns:
            tuple: A tuple containing:
                - new_mu (torch.Tensor): Flattened 'mu'.
                - new_eps (torch.Tensor): Flattened 'eps'.
        """
        mu = mu.flatten(self.start_dim, self.end_dim).to(device)
        eps = eps.flatten(self.start_dim, self.end_dim).to(device)

        return mu, eps

class IntervalBatchNorm:
    def __init__(self) -> None:
        """Initialize IntervalBatchNorm layer."""
        pass

    def forward(self, mu: torch.Tensor,
                eps: torch.Tensor,
                weight: torch.Tensor,
                bias: torch.Tensor,
                running_mean: torch.Tensor,
                running_var: torch.Tensor,
                stats_id: int,
                batch_norm_forward: Callable,
                device: Union[str, torch.device] = "cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Apply interval batch normalization using endpoint propagation and shared batch stats.

        Args:
            mu (torch.Tensor): Midpoint of the interval.
            eps (torch.Tensor): Radius of the interval.
            weight (torch.Tensor): Affine scale parameter (gamma).
            bias (torch.Tensor): Affine shift parameter (beta).
            running_mean (torch.Tensor): Running mean for inference (not used here).
            running_var (torch.Tensor): Running variance for inference (not used here).
            stats_id (int): Identifier for stats tracking in custom BN layers.
            batch_norm_forward (Callable): Callable to apply batch norm (should accept training=True).
            device (torch.device, optional): Device for calculations, either "cpu" or "cuda". Defaults to "cuda".

        Returns:
            tuple: A tuple containing:
                - new_mu (torch.Tensor): Transformed midpoint.
                - new_eps (torch.Tensor): Transformed radius (guaranteed â‰¥ 0).
        """
        mu = mu.to(device)
        eps = eps.to(device)
        weight = weight.to(device)
        bias = bias.to(device)

        lower = mu - eps
        upper = mu + eps

        x_cat = torch.cat([lower, upper], dim=0) 
        x_cat_bn = batch_norm_forward(
            x_cat,
            running_mean=running_mean,
            running_var=running_var,
            weight=weight,
            bias=bias,
            stats_id=stats_id,
        )

        # Safety: enforce contiguous format post-BN
        x_cat_bn = x_cat_bn.to(memory_format=torch.contiguous_format).contiguous()

        # Explicit slicing (avoid chunk)
        mid = x_cat_bn.size(0) // 2
        lower_bn = x_cat_bn[:mid].contiguous()
        upper_bn = x_cat_bn[mid:].contiguous()

        # Enforce valid bounds
        lower_bn, upper_bn = torch.minimum(lower_bn, upper_bn), torch.maximum(lower_bn, upper_bn)

        new_mu = (upper_bn + lower_bn) / 2
        new_eps = (upper_bn - lower_bn) / 2

        return new_mu, new_eps

    
class IntervalAvgPool2d:
    """Interval version of AvgPool2d.

    For detailed argument descriptions, see: https://pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool2d.html
    """

    def __init__(self, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = None,
                  padding: Union[int, Tuple[int, int]] = 0) -> None:
        """Initialize IntervalAvgPool2d layer.

        Args:
            kernel_size (Union[int, Tuple[int, int]]): Size of the pooling kernel.
            stride (Optional[Union[int, Tuple[int, int]]], optional): Stride of the pooling operation. Defaults to None.
            padding (Union[int, Tuple[int, int]], optional): Padding applied to the input. Defaults to 0.
        """
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, mu: torch.Tensor, eps: torch.Tensor, device: Union[str, torch.device] = "cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """Apply interval average-pooling.

        Args:
            mu (torch.Tensor): Midpoint of the interval.
            eps (torch.Tensor): Radius of the interval.
            device (Union[str, torch.device], optional): Device for computation, either "cpu" or "cuda". Defaults to "cuda".

        Returns:
            tuple: A tuple containing:
                - new_mu (torch.Tensor): Pooled midpoint.
                - new_eps (torch.Tensor): Pooled radius.
        """
        mu = mu.to(device)
        eps = eps.to(device)

        z_lower, z_upper = mu - eps, mu + eps
        z_lower = F.avg_pool2d(
            z_lower,
            self.kernel_size,
            self.stride,
            self.padding,
        )

        z_upper = F.avg_pool2d(
            z_upper,
            self.kernel_size,
            self.stride,
            self.padding,
        )

        new_mu, new_eps = (z_upper + z_lower) / 2.0, (z_upper - z_lower) / 2.0

        return new_mu, new_eps
    
class IntervalMaxPool2d:
    """Interval version of MaxPool2d.

    For detailed argument descriptions, see: https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html
    """

    def __init__(self, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]] = None,
                  padding: Union[int, Tuple[int, int]] = 0) -> None:
        """Initialize IntervalMaxPool2d layer.

        Args:
            kernel_size (Union[int, Tuple[int, int]]): Size of the pooling kernel.
            stride (Optional[Union[int, Tuple[int, int]]], optional): Stride of the pooling operation. Defaults to None.
            padding (Union[int, Tuple[int, int]], optional): Padding applied to the input. Defaults to 0.
        """
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

    def forward(self, mu: torch.Tensor, eps: torch.Tensor, device: Union[str, torch.device] = "cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """Apply interval max-pooling.

        Args:
            mu (torch.Tensor): Midpoint of the interval.
            eps (torch.Tensor): Radius of the interval.
            device (Union[str, torch.device], optional): Device for computation, either "cpu" or "cuda". Defaults to "cuda".

        Returns:
            tuple: A tuple containing:
                - new_mu (torch.Tensor): Pooled midpoint.
                - new_eps (torch.Tensor): Pooled radius.
        """
        mu = mu.to(device)
        eps = eps.to(device)

        z_lower, z_upper = mu - eps, mu + eps
        z_lower = F.max_pool2d(
            z_lower,
            self.kernel_size,
            self.stride,
            self.padding,
        )

        z_upper = F.max_pool2d(
            z_upper,
            self.kernel_size,
            self.stride,
            self.padding,
        )

        new_mu, new_eps = (z_upper + z_lower) / 2.0, (z_upper - z_lower) / 2.0

        return new_mu, new_eps

    
class IntervalDropout:
    """Interval version of Dropout.

    For detailed argument descriptions, see: https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout.html
    """

    def __init__(self, p: float = 0.5, inplace: bool = False) -> None:
        """Initialize IntervalDropout layer.

        Args:
            p (float, optional): Probability of an element to be zeroed. Defaults to 0.5.
            inplace (bool, optional): If set to True, will do this operation in-place. Defaults to False.
        """
        self.p = p
        self.inplace = inplace

    def forward(self, mu: torch.Tensor, eps: torch.Tensor, device: Union[str, torch.device] = "cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """Apply interval dropout.

        Args:
            mu (torch.Tensor): Midpoint of the interval.
            eps (torch.Tensor): Radius of the interval.
            device (Union[str, torch.device], optional): Device for computation, either "cpu" or "cuda". Defaults to "cuda".

        Returns:
            tuple: A tuple containing:
                - new_mu (torch.Tensor): Midpoint after dropout.
                - new_eps (torch.Tensor): Radius after dropout.
        """
        mu = mu.to(device)
        eps = eps.to(device)
        z_lower, z_upper = mu - eps, mu + eps

        mask = (torch.rand_like(mu) > self.p).float()
        z_lower = z_lower * mask / (1 - self.p)
        z_upper = z_upper * mask / (1 - self.p)

        new_mu = (z_lower + z_upper) / 2.0
        new_eps = (z_upper - z_lower) / 2.0
        return new_mu, new_eps