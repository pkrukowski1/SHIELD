import os
import torch
import logging
import pandas as pd
from omegaconf import DictConfig
from hydra.utils import instantiate
from tqdm import tqdm
import torch.nn.utils as nn_utils
from typing import Tuple

from utils.fabric import setup_fabric
from utils.handy_functions import prepare_weights
from model.model_abc import CLModuleABC

log = logging.getLogger(__name__)
log.setLevel(logging.INFO)

@torch.no_grad()
def prepare_dataset_for_check(dataset, fabric) -> Tuple[torch.Tensor, torch.Tensor, int]:
    """
    Prepares continual learning dataset.
    """
    inputs = dataset.get_test_inputs()
    targets = dataset.get_test_outputs()
    test_input = dataset.input_to_torch_tensor(inputs, fabric.device, mode="inference")
    test_target = dataset.output_to_torch_tensor(targets, fabric.device, mode="inference")
    test_target = test_target.max(dim=1)[1] # Get ground truth class indices

    assert test_input.ndim == 2, f"Expected flattened input [N, F], got shape {test_input.shape}"
    N = test_input.shape[0]
    
    return test_input, test_target, N

@torch.no_grad()
def compute_margins(model, dataset, task_id, hnet_weights_dict, epsilon, fabric) -> Tuple[float, torch.Tensor]:
    """
    Computes the mean certified margin (M_mean) and the vector of all
    per-sample margins for task s using the parameters generated by hnet_weights_dict.
    """
    model.hnet.load_state_dict(prepare_weights(hnet_weights_dict, model), strict=False)
    model.hnet.eval()
    
    test_input, test_target, N = prepare_dataset_for_check(dataset, fabric)

    all_margins = []
    batch_size = 512 

    pbar = tqdm(range(0, N, batch_size), desc="Computing margins", leave=False)
    for start in pbar:
        end = min(start + batch_size, N)
        batch_input = test_input[start:end]
        batch_target = test_target[start:end]

        out, eps = model(x=batch_input, task_id=task_id, epsilon=epsilon)

        lower_bounds = out - eps
        upper_bounds = out + eps

        true_class_lower_bound = lower_bounds.gather(1, batch_target.unsqueeze(1))
        other_class_upper_bounds = upper_bounds.clone()
        other_class_upper_bounds.scatter_(1, batch_target.unsqueeze(1), -float('inf'))
        max_other_upper_bound = other_class_upper_bounds.max(dim=1)[0]
        margins = true_class_lower_bound.squeeze(1) - max_other_upper_bound
        
        all_margins.append(margins)
    
    full_margins_vec = torch.cat(all_margins, dim=0)
    mean_margin = full_margins_vec.mean().item()
            
    return mean_margin, full_margins_vec

@torch.no_grad()
def get_all_logits(model, test_input, N, batch_size, task_id) -> torch.Tensor:
    """
    Helper function to compute logits for all inputs in batches.
    """
    all_logits = []
    for start in range(0, N, batch_size):
        end = min(start + batch_size, N)
        batch_input = test_input[start:end]
        # Use epsilon=0 to get the clean logits f(x), as in the theorem
        all_logits.append(model(x=batch_input, task_id=task_id, epsilon=0)[0])
    return torch.cat(all_logits, dim=0)

@torch.no_grad()
def compute_deltas(model, dataset, task_id, weights_initial, weights_final, fabric, config) -> Tuple[float, torch.Tensor]:
    """
    Computes Delta_max^(s) and the vector of all per-sample deltas.
    
    Delta(x) = ||f(x; theta_s(t_final)) - f(x; theta_s(s))||_inf
    Delta_max = max(Delta(x))
    """
    log.info(f"Computing deltas for s={task_id}...")
    test_input, _, N = prepare_dataset_for_check(dataset, fabric)
    batch_size = 512

    # 1. Get all logits using initial weights
    model.hnet.load_state_dict(prepare_weights(weights_initial, model), strict=False)
    model.eval()
    all_logits_initial = get_all_logits(model, test_input, N, batch_size, task_id)

    # 2. Get all logits using final weights
    model.hnet.load_state_dict(prepare_weights(weights_final, model), strict=False)
    model.eval()
    all_logits_final = get_all_logits(model, test_input, N, batch_size, task_id)
    
    # 3. Compute diff and L-inf norm for each sample
    diff_matrix = all_logits_final - all_logits_initial
    linf_norms_per_sample = torch.linalg.norm(diff_matrix, ord=float('inf'), dim=1)
    
    # 4. Find the max over all samples
    max_delta = linf_norms_per_sample.max().item()
    
    return max_delta, linf_norms_per_sample


def experiment(config: DictConfig) -> None:
    """
    Load all datasets and weights. Check theorem assumptions ONCE at the end.
    
    1. Checks the relaxed (mean) theorem condition.
    2. Calculates the percentage of samples that are *guaranteed* to
       maintain robustness based on the theorem's per-sample proof.
    """
    number_of_tasks = config.dataset.number_of_tasks
    epsilon = config.exp.epsilon
    path_to_weights = config.exp.path_to_weights

    log.info("Preparing datasets")
    cl_dataset = instantiate(config.dataset)
    task_datasets = cl_dataset.prepare_tasks(os.getenv("DATA_DIR"))

    log.info("Setting up Fabric")
    fabric = setup_fabric(config)

    log.info("Initializing model (CLModuleABC)")
    model = instantiate(config.model, number_of_tasks=number_of_tasks).to(fabric.device)
    
    log.info("Loading all hypernetwork weights")
    all_hnet_weights = []
    for i in range(number_of_tasks):
        try:
            weights_path = f"{path_to_weights}/hnet_after_{i+1}_task.pt"
            hnet_weights = torch.load(weights_path, map_location=fabric.device)
            all_hnet_weights.append(hnet_weights)
            log.info(f"Loaded weights from: {weights_path}")
        except FileNotFoundError:
            log.error(f"Could not find weights: {weights_path}")
            log.error("Aborting experiment.")
            return
            
    results = []
    
    if number_of_tasks < 2:
        log.warning("Need at least 2 tasks to check theorem. Exiting.")
        return

    t_final = number_of_tasks - 1
    weights_final = all_hnet_weights[t_final]
    
    log.info(f"--- Checking Theorem Against Final Task t={t_final} ---")

    for s in range(t_final):
        log.info(f"Checking condition for previous task s={s}...")
        
        weights_initial = all_hnet_weights[s]
        
        # --- 1. Compute all necessary per-sample vectors ---
        
        # Get mean margin AND per-sample margins at time t_final
        M_mean_final, margins_final = compute_margins(
            model, task_datasets[s], s, weights_final, epsilon, fabric
        )
        
        # Get mean margin AND per-sample margins at time s (initial)
        M_mean_initial, margins_initial = compute_margins(
            model, task_datasets[s], s, weights_initial, epsilon, fabric
        )
        
        # Get max delta AND per-sample deltas
        Delta_max, deltas_per_sample = compute_deltas(
            model, task_datasets[s], s, weights_initial, weights_final, fabric, config
        )
        
        # --- 2. Perform the "Relaxed Theorem" (mean/max) check ---
        condition_met_relaxed = Delta_max <= (0.5 * M_mean_final)
        
        # --- 3. Perform the "Guaranteed Percentage" (per-sample) check ---
        # This is the core logic from the theorem's proof:
        # M_new >= M_old - 2*Delta
        guaranteed_new_margins = margins_initial - (2.0 * deltas_per_sample)
        
        num_guaranteed_robust = (guaranteed_new_margins > 0).sum().item()
        N = margins_initial.shape[0]
        percent_guaranteed_robust = 100.0 * num_guaranteed_robust / N
        
        # (Optional but useful) Compute the ACTUAL final robustness percentage
        num_actually_robust = (margins_final > 0).sum().item()
        percent_actually_robust = 100.0 * num_actually_robust / N
        
        # --- 4. Log results ---
        log.info(f"--- Results for s={s} (comparing s -> t_final={t_final}) ---")
        log.info(f"[Relaxed Theorem] Delta_max: {Delta_max:.4e} | "
                 f"Budget (0.5*M_mean_final): {(0.5 * M_mean_final):.4e} | "
                 f"Met: {condition_met_relaxed}")
        
        log.info(f"[Per-Sample Check] Guaranteed Robust (by Thm): {percent_guaranteed_robust:.2f}% | "
                 f"Actual Final Robust: {percent_actually_robust:.2f}%")

        results.append({
            "s_previous_task": s,
            "t_final_task": t_final,
            "M_mean_s_t_final": M_mean_final,
            "Delta_max_s_t_final": Delta_max,
            "Relaxed_Condition_Met": condition_met_relaxed,
            "Percent_Guaranteed_Robust_by_Thm": percent_guaranteed_robust,
            "Percent_Actually_Robust_Final": percent_actually_robust
        })

    # Save final results to CSV
    results_df = pd.DataFrame(results)
    out_path = os.path.join(config.exp.log_dir, "theorem_robustness_analysis.csv")
    results_df.to_csv(out_path, sep=";", index=False)
    log.info(f"Saved theorem check results to: {out_path}")