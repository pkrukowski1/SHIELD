import torch
import torch.nn as nn
from hypnettorch.mnets.mnet_interface import MainNetInterface
from hypnettorch.utils.torch_utils import init_params

class IntervalMLP(nn.Module, MainNetInterface):
    def __init__(
        self,
        n_in=1,
        n_out=1,
        hidden_layers=(10, 10),
        activation_fn=torch.nn.ReLU(),
        use_bias=True,
        no_weights=True,
        out_fn=None,
    ):
        # FIXME find a way using super to handle multiple inheritance.
        nn.Module.__init__(self)
        MainNetInterface.__init__(self)

        assert no_weights is True, "Learnable parameters are only generated by a hypernetwork"

        hidden_layers = list(hidden_layers)

        self._a_fun = activation_fn
        self._no_weights = no_weights
        self._out_fn = out_fn

        self._has_bias = use_bias
        self._has_fc_out = True
        self._mask_fc_out = True
        self._has_linear_out = True

        self._param_shapes = []
        self._param_shapes_meta = []
        self._weights = (
            nn.ParameterList()
        )

        ### Compute shapes of linear layers.
        linear_shapes = IntervalMLP.weight_shapes(
            n_in=n_in,
            n_out=n_out,
            hidden_layers=hidden_layers,
            use_bias=use_bias,
        )
        self._param_shapes.extend(linear_shapes)

        for i, s in enumerate(linear_shapes):
            self._param_shapes_meta.append(
                {
                    "name": "weight" if len(s) != 1 else "bias",
                    "index": -1 if no_weights else len(self._weights) + i,
                    "layer": -1,  # 'layer' is set later.
                }
            )

        layer_ind = 0
        for i, dd in enumerate(self._param_shapes_meta):
            dd["layer"] = 1 + layer_ind
            if not use_bias or dd["name"] == "bias":
                layer_ind += 1

        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()
        for i, dims in enumerate(linear_shapes):
            self._weights.append(
                nn.Parameter(torch.Tensor(*dims), requires_grad=True)
            )
            if len(dims) == 1:
                self._layer_bias_vectors.append(self._weights[-1])
            else:
                self._layer_weight_tensors.append(self._weights[-1])

        for i in range(len(self._layer_weight_tensors)):
            if use_bias:
                init_params(
                    self._layer_weight_tensors[i],
                    self._layer_bias_vectors[i],
                )
            else:
                init_params(self._layer_weight_tensors[i])


        self._is_properly_setup()

    def forward(self, x, epsilon, weights=None, distilled_params=None, condition=None):
        if weights is None:
           weights = self.weights

        w_weights = []
        b_weights = []

        for i, p in enumerate(weights):
            if self.has_bias and i % 2 == 1:
                b_weights.append(p)
            else:
                w_weights.append(p)
        hidden = x
        eps = (epsilon * torch.ones_like(hidden)).T
        for l in range(len(w_weights)):
            W = w_weights[l]
            b = b_weights[l] if self.has_bias else None

            hidden = hidden @ W.T
            eps = torch.abs(W) @ eps
            if b is not None:
                hidden = hidden + b

            if l < len(w_weights) - 1:
                z_lower = self._a_fun(hidden - eps.T)
                z_upper = self._a_fun(hidden + eps.T)
                hidden = (z_upper + z_lower) / 2
                eps = (z_upper - z_lower) / 2
                eps = eps.T
        return hidden, eps.T

    def distillation_targets(self):
        return None

    @staticmethod
    def weight_shapes(n_in=1, n_out=1, hidden_layers=[10, 10], use_bias=True):
        shapes = []
        prev_dim = n_in
        layer_out_sizes = hidden_layers + [n_out]
        for i, size in enumerate(layer_out_sizes):
            shapes.append([size, prev_dim])
            if use_bias:
                shapes.append([size])
            prev_dim = size

        return shapes


if __name__ == "__main__":
    pass
