import torch.nn as nn
import numpy as np

from hypnettorch.mnets.classifier_interface import Classifier
from hypnettorch.mnets.mnet_interface import MainNetInterface

from utils.interval_modules import *


class IntervalAlexNet(Classifier):
    """Implementation of interval AlexNet.

    Only CIFAR-100 dataset is supported right now.

    Parameters:
    -----------
        in_shape: tuple or list
            The shape of an input sample.
            .. note::
                We assume the Tensorflow format, where the last entry
                denotes the number of channels.
        num_classes: int
            The number of output neurons.
        arch: str
            A neural network architecture. Only CIFAR-100 is supported
            right now. The only possible default value is 'cifar'.
        verbose: bool
            Allow printing of general information about the
            generated network (such as number of weights).
        no_weights: bool
            If set to ``True``, no trainable parameters will be
            constructed, i.e., weights are assumed to be produced ad-hoc
            by a hypernetwork and passed to the :meth:`forward` method.
        init_weights: optional
            This option is for convinience reasons.
            The option expects a list of parameter values that are used to
            initialize the network weights. As such, it provides a
            convinient way of initializing a network with a weight draw
            produced by the hypernetwork.
        bn_track_stats: bool
            If is set to False, this layer then does not keep running estimates
            and batch statistics are instead used during evaluation time as well. For more information
            please see docs https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html
        distill_bn_stats: bool
        If ``True``, then the shapes of the batchnorm statistics will be added to the attribute
            :attr:`mnets.mnet_interface.MainNetInterface.hyper_shapes_distilled` and the current statistics 
            will be returned by the method :meth:`distillation_targets`. Currently it is not used.

    Returns:
    --------
        torch.Tensor
    """

    def __init__(
        self,
        in_shape=(32, 32, 3),
        num_classes=10,
        verbose=True,
        arch="cifar",
        no_weights=True,
        use_batch_norm=True,
        bn_track_stats=True,
        distill_bn_stats=False,
        init_weights=None,
    ):
        super(IntervalAlexNet, self).__init__(num_classes, verbose)

        assert no_weights is True, "Learnable parameters are only generated by a hypernetwork"

        _architectures = {
            "cifar": [
                [32, 3, 3, 3],              
                [32],
                [64, 32, 3, 3],             
                [64],
                [128, 64, 3, 3],            
                [128],
                [128, 128, 3, 3],           
                [128],
                [128, 128, 3, 3],           
                [128],

                [512, 128 * 2 * 2],         
                [512],
                [512, 512],                 
                [512],
                [num_classes, 512],        
                [num_classes]
            ]
        }

        if arch == "cifar":
            assert in_shape[0] == 32 and in_shape[1] == 32
        else:
            raise ValueError(
                "Dataset other than CIFAR are " "not handled!"
            )
        self._in_shape = in_shape

        self._hyper_shapes_learned = (
            None if not no_weights and not self._context_mod_no_weights else []
        )
        self._hyper_shapes_learned_ref = (
            None if self._hyper_shapes_learned is None else []
        )

        self.architecture = arch
        assert self.architecture in _architectures.keys()
        self._param_shapes = _architectures[self.architecture]
        self._param_shapes[-2][0] = num_classes
        self._param_shapes[-1][0] = num_classes

        assert init_weights is None or no_weights is False
        self._no_weights = no_weights

        self._has_bias = True
        self._has_fc_out = True
        # We need to make sure that the last 2 entries of `weights` correspond
        # to the weight matrix and bias vector of the last layer.
        self._mask_fc_out = True
        # We don't use any output non-linearity.
        self._has_linear_out = True
        self._use_batch_norm = use_batch_norm
        self._bn_track_stats = bn_track_stats

        # Add BatchNorm layers at the end
        if use_batch_norm:
            
            start_idx = len(_architectures[self.architecture])

            bn_sizes = [
                32, 64, 128, 128, 128, 512, 512
            ]

            bn_layers = list(range(start_idx, start_idx + len(bn_sizes)))
            self._bn_params_start_idx = start_idx

            self._add_batchnorm_layers(
                bn_sizes,
                no_weights,
                bn_layers=bn_layers,
                distill_bn_stats=distill_bn_stats,
                bn_track_stats=bn_track_stats,
            )

        self._num_weights = MainNetInterface.shapes_to_num_weights(
            self._param_shapes
        )
        if verbose:
            print(
                "Creating an AlexNet with %d weights" % (self._num_weights)
            )

        self._weights = None
        self._hyper_shapes_learned = self._param_shapes
        self._hyper_shapes_learned_ref = list(
            range(len(self._param_shapes))
        )

        self.layers = [
            IntervalConv2d(3, 32, 3, stride=2, padding=1),
            IntervalMaxPool2d(2),
            IntervalReLU(),
            IntervalBatchNorm(),

            IntervalConv2d(32, 64, 3, padding=1),
            IntervalMaxPool2d(2),
            IntervalReLU(),
            IntervalBatchNorm(),

            IntervalConv2d(64, 128, 3, padding=1),
            IntervalReLU(),
            IntervalBatchNorm(),

            IntervalConv2d(128, 128, 3, padding=1),
            IntervalReLU(),
            IntervalBatchNorm(),

            IntervalConv2d(128, 128, 3, padding=1),
            IntervalMaxPool2d(2),
            IntervalReLU(),
            IntervalBatchNorm(),

            IntervalFlatten(),

            IntervalLinear(128 * 2 * 2, 512),
            IntervalReLU(),
            IntervalBatchNorm(),

            IntervalLinear(512, 512),
            IntervalReLU(),
            IntervalBatchNorm(),

            IntervalLinear(512, num_classes)
        ]

        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()

        self._is_properly_setup()

    def forward(self, x, epsilon, weights, distilled_params=None, condition=None):
        """Compute the output :math:`y` of this network given the input
        :math:`x`.

        Parameters:
        -----------
            (....): See docstring of method
                :meth:`mnets.mnet_interface.MainNetInterface.forward`.
            x: torch.Tensor
                Input image (Tensorflow format: last entry denotes channels).
            epsilon: torch.Tensor
                Radii of a hypercube around x.

        Returns:
        ---------
            tuple: (mu, eps)
                The output mean and epsilon of the network.
        """
        if distilled_params is not None:
            raise ValueError("distilled_params not implemented for this network!")
        
        if self._no_weights and weights is None:
            raise Exception("Network generated without weights. weights option may not be None.")
        
        # Validate weights
        shapes = self.param_shapes
        assert len(weights) == len(shapes)
        for i, s in enumerate(shapes):
            assert np.all(np.equal(s, list(weights[i].shape)))

        # Parse conditions
        bn_cond = None
        if condition is not None:
            if isinstance(condition, dict):
                if "bn_stats_id" in condition.keys():
                    bn_cond = condition["bn_stats_id"]
                if "cmod_ckpt_id" in condition.keys():
                    raise ValueError("Context modulation layers are not used!")
            else:
                bn_cond = condition

        # Prepare batchnorm stats
        running_means = [None] * len(self._batchnorm_layers) if self._use_batch_norm else []
        running_vars = [None] * len(self._batchnorm_layers) if self._use_batch_norm else []
        
        if self._use_batch_norm and self._bn_track_stats and bn_cond is None:
            for i, bn_layer in enumerate(self._batchnorm_layers):
                running_means[i], running_vars[i] = bn_layer.get_stats()

        # Initialize forward pass
        x = x.view(-1, *self._in_shape)
        x = x.permute(0, 3, 1, 2)  # Convert to PyTorch format (N, C, H, W)
        mu, eps = x, epsilon * torch.ones_like(x)
        
        weight_idx = 0  # Track current weight index
        bn_idx = 0     # Track current batchnorm index

        for layer in self.layers:
            device = mu.device
            if isinstance(layer, (IntervalConv2d, IntervalLinear)):
                # Use consecutive weights for weight matrix and bias
                weight = weights[weight_idx]
                bias = weights[weight_idx + 1]
                mu, eps = layer.forward(mu, eps, weight, bias=bias, device=device)
                weight_idx += 2
                
            elif self._use_batch_norm and isinstance(layer, IntervalBatchNorm):
                # Use batchnorm parameters
                weight = weights[self._bn_params_start_idx + bn_idx * 2]
                bias = weights[self._bn_params_start_idx + bn_idx * 2 + 1]
                rm = running_means[bn_idx]
                rv = running_vars[bn_idx]
                batch_norm_forward = self._batchnorm_layers[bn_idx].forward
                
                mu, eps = layer.forward(
                    mu, eps, weight, bias,
                    running_mean=rm,
                    running_var=rv,
                    stats_id=bn_cond,
                    batch_norm_forward=batch_norm_forward,
                    device=device
                )
                bn_idx += 1
                
            elif isinstance(layer, (IntervalReLU, IntervalMaxPool2d, IntervalFlatten)):
                mu, eps = layer.forward(mu, eps, device=device)
        return mu, eps

    def distillation_targets(self):
        """Targets to be distilled after training.

        See docstring of abstract super method
        :meth:`mnets.mnet_interface.MainNetInterface.distillation_targets`.

        This network does not have any distillation targets.

        Returns:
            ``None``
        """
        return None