import torch.nn as nn

import numpy as np
from typing import Tuple, List

from hypnettorch.mnets.classifier_interface import Classifier
from hypnettorch.mnets.mnet_interface import MainNetInterface

from utils.interval_modules import *

class ReducedIntervalAlexNet(Classifier):
    """Implementation of reduced interval AlexNet.

    Only the CIFAR dataset is supported right now.

    Args
        in_shape : tuple or list, optional
            The shape of an input sample. Assumes TensorFlow format (H, W, C).
        num_classes : int, optional
            The number of output neurons/classes.
        verbose : bool, optional
            If True, prints general information about the generated network.
        arch : str, optional
            Neural network architecture. Only 'cifar' is supported.
        no_weights : bool, optional
            If True, no trainable parameters are constructed; weights are expected
            to be provided externally (e.g., by a hypernetwork).
        use_batch_norm : bool, optional
            If True, uses batch normalization layers.
        bn_track_stats : bool, optional
            If False, batchnorm layers do not keep running estimates and use batch
            statistics during evaluation. See PyTorch docs for details.
        distill_bn_stats : bool, optional
            If True, batchnorm statistics shapes are added to
            `hyper_shapes_distilled` and returned by `distillation_targets`.
            Currently not used.
        init_weights : list, optional
            If provided, initializes network weights with these values. Only used
            if `no_weights` is False.

    """

    def __init__(
        self,
        in_shape: Tuple[int,int,int]=(32, 32, 3),
        num_classes: int=10,
        verbose: bool=True,
        arch: str="cifar",
        no_weights: bool=True,
        use_batch_norm: bool=True,
        bn_track_stats: bool=True,
        distill_bn_stats: bool=False,
        init_weights: List=None,
    ) -> None:
        super(ReducedIntervalAlexNet, self).__init__(num_classes, verbose)

        assert no_weights is True, "Learnable parameters are only generated by a hypernetwork"

        _architectures = {
            "cifar": [
                [32, 3, 3, 3],              
                [32],
                [64, 32, 3, 3],             
                [64],
                [128, 64, 3, 3],            
                [128],
                [100, 128],        
                [100],
                [num_classes, 100],        
                [num_classes]
            ]
        }

        if arch == "cifar":
            assert in_shape[0] == 32 and in_shape[1] == 32
        else:
            raise ValueError(
                "Dataset other than CIFAR are " "not handled!"
            )
        self._in_shape = in_shape

        self._hyper_shapes_learned = (
            None if not no_weights and not self._context_mod_no_weights else []
        )
        self._hyper_shapes_learned_ref = (
            None if self._hyper_shapes_learned is None else []
        )

        self.architecture = arch
        assert self.architecture in _architectures.keys()
        self._param_shapes = _architectures[self.architecture]
        self._param_shapes[-2][0] = num_classes
        self._param_shapes[-1][0] = num_classes

        assert init_weights is None or no_weights is False
        self._no_weights = no_weights

        self._has_bias = True
        self._has_fc_out = True
        # We need to make sure that the last 2 entries of `weights` correspond
        # to the weight matrix and bias vector of the last layer.
        self._mask_fc_out = True
        # We don't use any output non-linearity.
        self._has_linear_out = True
        self._use_batch_norm = use_batch_norm
        self._bn_track_stats = bn_track_stats

        # Add BatchNorm layers at the end
        if use_batch_norm:
            
            start_idx = len(_architectures[self.architecture])

            bn_sizes = [
                32, 64, 128, 100, #512, 512
            ]

            bn_layers = list(range(start_idx, start_idx + len(bn_sizes)))
            self._bn_params_start_idx = start_idx

            self._add_batchnorm_layers(
                bn_sizes,
                no_weights,
                bn_layers=bn_layers,
                distill_bn_stats=distill_bn_stats,
                bn_track_stats=bn_track_stats,
            )

        self._num_weights = MainNetInterface.shapes_to_num_weights(
            self._param_shapes
        )
        if verbose:
            print(
                "Creating an AlexNet with %d weights" % (self._num_weights)
            )

        self._weights = None
        self._hyper_shapes_learned = self._param_shapes
        self._hyper_shapes_learned_ref = list(
            range(len(self._param_shapes))
        )

        self.layers = [
            IntervalConv2d(3, 32, 4),
            IntervalReLU(),
            IntervalBatchNorm(),
            IntervalAvgPool2d(2),

            IntervalConv2d(32, 64, 3),
            IntervalReLU(),
            IntervalBatchNorm(),
            IntervalAvgPool2d(2),

            IntervalConv2d(64, 128, 2),
            IntervalReLU(),
            IntervalBatchNorm(),
            IntervalAvgPool2d(4),

            IntervalFlatten(),

            IntervalLinear(128, 100),
            IntervalReLU(),
            IntervalBatchNorm(),

            IntervalLinear(100, num_classes)
        ]

        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()

        self._is_properly_setup()

    def forward(self, x: torch.Tensor, epsilon: float, weights: List[torch.Tensor], distilled_params=None, 
                condition: int=None) -> Tuple[torch.Tensor,torch.Tensor]:
        """Compute network output given input, epsilon, and weights.

        Args:
            x (torch.Tensor): Input (N, H, W, C).
            epsilon (float): Perturbation radius.
            weights (list[torch.Tensor]): Network weights/biases.
            distilled_params: Unused.
            condition: BatchNorm stats condition.

        Returns:
            (mu, eps): Output midpoint and radii tensors.
        """
        if distilled_params is not None:
            raise ValueError("distilled_params not implemented for this network!")
        
        if self._no_weights and weights is None:
            raise Exception("Network generated without weights. weights option may not be None.")
        
        # Validate weights
        shapes = self.param_shapes
        assert len(weights) == len(shapes)
        for i, s in enumerate(shapes):
            assert np.all(np.equal(s, list(weights[i].shape)))

        # Parse conditions
        bn_cond = None
        if condition is not None:
            if isinstance(condition, dict):
                if "bn_stats_id" in condition.keys():
                    bn_cond = condition["bn_stats_id"]
                if "cmod_ckpt_id" in condition.keys():
                    raise ValueError("Context modulation layers are not used!")
            else:
                bn_cond = condition

        # Prepare batchnorm stats
        running_means = [None] * len(self._batchnorm_layers) if self._use_batch_norm else []
        running_vars = [None] * len(self._batchnorm_layers) if self._use_batch_norm else []
        
        if self._use_batch_norm and self._bn_track_stats and bn_cond is None:
            for i, bn_layer in enumerate(self._batchnorm_layers):
                running_means[i], running_vars[i] = bn_layer.get_stats()

        # Initialize forward pass
        x = x.view(-1, *self._in_shape)
        x = x.permute(0, 3, 1, 2)  # Convert to PyTorch format (N, C, H, W)
        mu, eps = x, epsilon * torch.ones_like(x)
        
        weight_idx = 0  # Track current weight index
        bn_idx = 0     # Track current batchnorm index

        for layer in self.layers:
            device = mu.device
            if isinstance(layer, (IntervalConv2d, IntervalLinear)):
                # Use consecutive weights for weight matrix and bias
                weight = weights[weight_idx]
                bias = weights[weight_idx + 1]
                mu, eps = layer.forward(mu, eps, weight, bias=bias, device=device)
                weight_idx += 2
                
            elif self._use_batch_norm and isinstance(layer, IntervalBatchNorm):
                # Use batchnorm parameters
                weight = weights[self._bn_params_start_idx + bn_idx * 2]
                bias = weights[self._bn_params_start_idx + bn_idx * 2 + 1]
                rm = running_means[bn_idx]
                rv = running_vars[bn_idx]
                batch_norm_forward = self._batchnorm_layers[bn_idx].forward
                
                mu, eps = layer.forward(
                    mu, eps, weight, bias,
                    running_mean=rm,
                    running_var=rv,
                    stats_id=bn_cond,
                    batch_norm_forward=batch_norm_forward,
                    device=device
                )
                bn_idx += 1
                
            elif isinstance(layer, (IntervalAvgPool2d, IntervalFlatten, IntervalReLU)):
                mu, eps = layer.forward(mu, eps, device=device)
        return mu, eps

    def distillation_targets(self):
        """
        Returns targets to be distilled after training.

        Overrides the abstract method from MainNetInterface.

        This network does not have any distillation targets.

        Returns:
            None
        """
        return None