"""The implementation is based on https://hypnettorch.readthedocs.io/en/latest/_modules/hypnettorch/mnets/mlp.html#MLP"""

import torch
import torch.nn as nn

from hypnettorch.mnets.mnet_interface import MainNetInterface
from hypnettorch.utils.torch_utils import init_params

from typing import Tuple, List, Callable

class IntervalMLP(nn.Module, MainNetInterface):
    """
    IntervalMLP is a multi-layer perceptron (MLP) designed for interval bound propagation, supporting non-learnable weights for use with hypernetworks.
    This class implements a feedforward neural network with configurable input/output sizes, hidden layers, activation functions, and bias usage. 
    It is intended for use in scenarios where weights are provided externally (e.g., by a hypernetwork), and thus does not create learnable parameters by default.
    Inheritance:
        nn.Module: PyTorch base class for all neural network modules.
        MainNetInterface: Custom interface for main network behavior.
    Args:
        n_in (int): Number of input features. Default is 1.
        n_out (int): Number of output features. Default is 1.
        hidden_layers (tuple): Sizes of hidden layers. Default is (10, 10).
        activation_fn (callable): Activation function to use between layers. Default is torch.nn.ReLU().
        use_bias (bool): Whether to include bias terms in linear layers. Default is True.
        no_weights (bool): If True, the network does not create learnable parameters (weights must be provided externally). Default is True.
        out_fn (callable, optional): Optional output transformation function. Default is None.
    Attributes:
        _a_fun (callable): Activation function.
        _no_weights (bool): Indicates if weights are not learnable.
        _out_fn (callable): Output transformation function.
        _has_bias (bool): Indicates if bias is used.
        _has_fc_out (bool): Indicates if the network has a fully connected output layer.
        _mask_fc_out (bool): Indicates if the output layer is masked.
        _has_linear_out (bool): Indicates if the output layer is linear.
        _param_shapes (list): List of parameter shapes for each layer.
        _param_shapes_meta (list): Metadata for each parameter shape.
        _weights (nn.ParameterList): List of network parameters (weights and biases).
        _layer_weight_tensors (nn.ParameterList): List of weight tensors for each layer.
        _layer_bias_vectors (nn.ParameterList): List of bias vectors for each layer.
    Methods:
        forward(x, epsilon, weights=None, distilled_params=None, condition=None):
            Performs a forward pass with interval bound propagation, returning the output and propagated epsilon.
        distillation_targets():
            Returns distillation targets (None for this implementation).
        weight_shapes(n_in=1, n_out=1, hidden_layers=[10, 10], use_bias=True):
            Static method to compute the shapes of weights and biases for each layer.
    Notes:
        - The network is designed to work with externally provided weights (e.g., from a hypernetwork).
        - The forward method propagates both the input and an interval epsilon through the network, supporting interval bound propagation for robustness analysis.
        - Biases are handled according to the use_bias flag.
        - The network asserts that no_weights is True, enforcing the use of external weights.
    """
    
    def __init__(
        self,
        n_in: int=1,
        n_out: int=1,
        hidden_layers: Tuple=(10, 10),
        activation_fn: nn.Module=torch.nn.ReLU(),
        use_bias: bool=True,
        no_weights: bool=True,
        out_fn: Callable=None,
    ) -> None:
        # FIXME find a way using super to handle multiple inheritance.
        nn.Module.__init__(self)
        MainNetInterface.__init__(self)

        assert no_weights is True, "Learnable parameters are only generated by a hypernetwork"

        hidden_layers = list(hidden_layers)

        self._a_fun = activation_fn
        self._no_weights = no_weights
        self._out_fn = out_fn

        self._has_bias = use_bias
        self._has_fc_out = True
        self._mask_fc_out = True
        self._has_linear_out = True

        self._param_shapes = []
        self._param_shapes_meta = []
        self._weights = (
            nn.ParameterList()
        )

        ### Compute shapes of linear layers.
        linear_shapes = IntervalMLP.weight_shapes(
            n_in=n_in,
            n_out=n_out,
            hidden_layers=hidden_layers,
            use_bias=use_bias,
        )
        self._param_shapes.extend(linear_shapes)

        for i, s in enumerate(linear_shapes):
            self._param_shapes_meta.append(
                {
                    "name": "weight" if len(s) != 1 else "bias",
                    "index": -1 if no_weights else len(self._weights) + i,
                    "layer": -1,  # 'layer' is set later.
                }
            )

        layer_ind = 0
        for i, dd in enumerate(self._param_shapes_meta):
            dd["layer"] = 1 + layer_ind
            if not use_bias or dd["name"] == "bias":
                layer_ind += 1

        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()
        for i, dims in enumerate(linear_shapes):
            self._weights.append(
                nn.Parameter(torch.Tensor(*dims), requires_grad=True)
            )
            if len(dims) == 1:
                self._layer_bias_vectors.append(self._weights[-1])
            else:
                self._layer_weight_tensors.append(self._weights[-1])

        for i in range(len(self._layer_weight_tensors)):
            if use_bias:
                init_params(
                    self._layer_weight_tensors[i],
                    self._layer_bias_vectors[i],
                )
            else:
                init_params(self._layer_weight_tensors[i])


        self._is_properly_setup()

    def forward(self, x: torch.Tensor, epsilon: float, weights: List[torch.Tensor]=None, 
                distilled_params=None, condition: int=None) -> Tuple[torch.Tensor,torch.Tensor]:
        """
        Perform a forward pass with interval bound propagation.

        Args:
            x (torch.Tensor): Input tensor.
            epsilon (float): Radii of a hypercube around x.
            weights (list of torch.Tensor, optional): List of weight and bias tensors. If None, uses self.weights.
            distilled_params: Unused. For interface compatibility.
            condition: Unused. For interface compatibility.

        Returns:
            tuple: (mu, eps)
            mu (torch.Tensor): Output midpoint after propagation.
            eps (torch.Tensor): Output radii after propagation.
        """
        if weights is None:
           weights = self.weights

        x = x.flatten(start_dim=1)

        w_weights = []
        b_weights = []

        for i, p in enumerate(weights):
            if self.has_bias and i % 2 == 1:
                b_weights.append(p)
            else:
                w_weights.append(p)
        hidden = x
        eps = (epsilon * torch.ones_like(hidden)).T
        for l in range(len(w_weights)):
            W = w_weights[l]
            b = b_weights[l] if self.has_bias else None

            hidden = hidden @ W.T
            eps = torch.abs(W) @ eps
            if b is not None:
                hidden = hidden + b

            if l < len(w_weights) - 1:
                z_lower = self._a_fun(hidden - eps.T)
                z_upper = self._a_fun(hidden + eps.T)
                hidden = (z_upper + z_lower) / 2
                eps = (z_upper - z_lower) / 2
                eps = eps.T
        return hidden, eps.T

    def distillation_targets(self):
        return None

    @staticmethod
    def weight_shapes(n_in: int=1, n_out: int=1, hidden_layers: Tuple[int]=[10, 10], use_bias: bool=True):
        """
        Compute the shapes of weights and biases for each layer of the MLP.

        Args:
            n_in (int, optional): Number of input features. Defaults to 1.
            n_out (int, optional): Number of output features. Defaults to 1.
            hidden_layers (list or tuple of int, optional): Sizes of hidden layers. Defaults to [10, 10].
            use_bias (bool, optional): Whether to include bias shapes for each layer. Defaults to True.

        Returns:
            list: List of shapes for weights and (optionally) biases for each layer, in order.
              Each weight shape is [out_features, in_features], and each bias shape is [out_features].
        """
        shapes = []
        prev_dim = n_in
        layer_out_sizes = hidden_layers + [n_out]
        for i, size in enumerate(layer_out_sizes):
            shapes.append([size, prev_dim])
            if use_bias:
                shapes.append([size])
            prev_dim = size

        return shapes


if __name__ == "__main__":
    pass
