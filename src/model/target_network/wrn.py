"""
Wide-ResNet
-----------

The module :mod:`mnets.wide_resnet` implements the class of Wide Residual
Networks as described in:

    Zagoruyko et al.,
    `"Wide Residual Networks" <https://arxiv.org/abs/1605.07146>`__, 2017.

The implementation is based on the file: https://hypnettorch.readthedocs.io/en/latest/_modules/hypnettorch/mnets/wide_resnet.html#WRN
"""
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from warnings import warn
from typing import List, Optional

from hypnettorch.mnets.classifier_interface import Classifier
from hypnettorch.mnets.mnet_interface import MainNetInterface
from hypnettorch.utils.torch_utils import init_params

from utils.interval_modules import *

class IntervalWRN(Classifier):
    """
    Hypernet-compatible Wide Residual Network (WRN).

    Implements a Wide Residual Network architecture, *WRN-d-k-B(3,3)*, following
    the original paper `Wide Residual Networks <https://arxiv.org/abs/1605.07146>`__.

    The architecture uses the following notation:
    - $l$: Deepening factor (layers per residual block), fixed at **2**.
    - $k$: **Widening factor** (multiplies feature map count).
    - $B(3,3)$: Block structure using **$3 \times 3$ quadratic kernels**.
    - $d$: Total number of convolutional layers (total depth is $6n+2$).
    - $n$: Number of residual blocks per group (network has 3 groups).

    Notable Implementation Differences to ResNet:
    - **Block Structure:** Convolutional layers are preceded by Batch Normalization (BN) and ReLU: $\text{BN} \rightarrow \text{ReLU} \rightarrow \text{Conv}$.
    - **Dropout:** Applied after the first convolutional layer of each residual block.
    - **Skip Connections:** $1 \times 1$ convolutions are used for skip connections when feature maps differ or downsampling is required.

    Args:
        in_shape (tuple or list): The shape of an input sample in ``HWC`` format, e.g., (32, 32, 3).
        num_classes (int): The number of output neurons (logits).
        n (int): Number of residual blocks per group.
        k (int): Widening factor applied to feature maps in the 3 main convolutional groups.
        num_feature_maps (tuple): List of 4 integers defining initial and group feature map counts. The last 3 entries are multiplied by $k$.
        use_bias (bool): Whether layers may have bias terms. Bias is redundant in conv layers with BN.
        use_fc_bias (optional, bool): Controls bias in the final fully-connected layer. If ``None``, it links to ``use_bias``.
        no_weights (bool): If ``True``, assumes weights are supplied ad-hoc by a hypernetwork (no trainable parameters are constructed, and BN affine is disabled).
        use_batch_norm (bool): Whether batch normalization should be used.
        bn_track_stats (bool): Whether to track running statistics for BN layers.
        distill_bn_stats (bool): Allows distillation of BN statistics.
        dropout_rate (float): The dropout rate (0 to 1). If $-1$, no dropout is applied.
        chw_input_format (bool): If ``True``, expects unflattened images in the ``CHW`` (PyTorch) format. Default assumes ``HWC``.
        verbose (bool): Allow printing of general information about the network (e.g., number of weights).
        **kwargs: Keyword arguments for context modulation.
    """
    def __init__(
        self,
        in_shape: tuple = (32, 32, 3),
        num_classes: int = 10,
        n: int = 4,
        k: int = 10,
        num_feature_maps: tuple = (16, 16, 32, 64),
        use_bias: bool = True,
        use_fc_bias: bool = None,
        no_weights: bool = False,
        use_batch_norm: bool = True,
        bn_track_stats: bool = True,
        distill_bn_stats: bool = False,
        dropout_rate: float = -1,
        chw_input_format: bool = False,
        verbose: bool = True,
        **kwargs
    ) -> None:
        
        super(IntervalWRN, self).__init__(num_classes, verbose)

        assert no_weights, "Learnable parameters are only generated by a hypernetwork"

        ### Parse or set context-mod arguments ###
        rem_kwargs = MainNetInterface._parse_context_mod_args(kwargs)
        if 'context_mod_apply_pixel_wise' in rem_kwargs:
            rem_kwargs.remove('context_mod_apply_pixel_wise')
        if len(rem_kwargs) > 0:
            raise ValueError('Keyword arguments %s unknown.' % str(rem_kwargs))
        # Since this is a conv-net, we may also want to add the following.
        if 'context_mod_apply_pixel_wise' not in kwargs.keys():
            kwargs['context_mod_apply_pixel_wise'] = False

        self._use_context_mod = kwargs['use_context_mod']
        self._context_mod_inputs = kwargs['context_mod_inputs']
        self._no_last_layer_context_mod = kwargs['no_last_layer_context_mod']
        self._context_mod_no_weights = kwargs['context_mod_no_weights']
        self._context_mod_post_activation = \
            kwargs['context_mod_post_activation']
        self._context_mod_gain_offset = kwargs['context_mod_gain_offset']
        self._context_mod_gain_softplus = kwargs['context_mod_gain_softplus']
        self._context_mod_apply_pixel_wise = \
            kwargs['context_mod_apply_pixel_wise']

        ### Check or parse remaining arguments ###
        self._in_shape = in_shape
        self._n = n
        self._k = k
        if use_fc_bias is None:
            use_fc_bias = use_bias
        # Also, checkout attribute `_has_bias` below.
        self._use_bias = use_bias
        self._use_fc_bias = use_fc_bias
        self._no_weights = no_weights
        assert not use_batch_norm or (not distill_bn_stats or bn_track_stats)
        self._use_batch_norm = use_batch_norm
        self._bn_track_stats = bn_track_stats
        self._distill_bn_stats = distill_bn_stats and use_batch_norm
        self._dropout_rate = dropout_rate
        self._chw_input_format = chw_input_format

        # The original authors found that the best configuration uses this
        # kernel in all convolutional layers.
        self._kernel_size = (3, 3)
        if len(num_feature_maps) != 4:
            raise ValueError('Option "num_feature_maps" must be a list of 4 ' +
                             'integers.')
        self._filter_sizes = list(num_feature_maps)
        if k != 1:
            for i in range(1, 4):
                self._filter_sizes[i] = k * num_feature_maps[i]
        # Strides used in the first layer of each convolutional group.
        self._strides = (1, 1, 2, 2)

        ### Set required class attributes ###
        # Note, we did overwrite the getter for attribute `has_bias`, as it is
        # not applicable if the values of `use_bias` and `use_fc_bias` differ.
        self._has_bias = use_bias if use_bias == use_fc_bias else False
        self._has_fc_out = True
        # We need to make sure that the last 2 entries of `weights` correspond
        # to the weight matrix and bias vector of the last layer!
        self._mask_fc_out = True
        self._has_linear_out = True

        self._param_shapes = []
        self._param_shapes_meta = []
        self._internal_params = None if no_weights and \
            self._context_mod_no_weights else nn.ParameterList()
        self._hyper_shapes_learned = None \
            if not no_weights and not self._context_mod_no_weights else []
        self._hyper_shapes_learned_ref = None if self._hyper_shapes_learned \
            is None else []
        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()

        if dropout_rate != -1:
            assert dropout_rate >= 0. and dropout_rate <= 1.
            self._dropout = nn.Dropout(p=dropout_rate)

        #################################
        ### Create context mod layers ###
        #################################
        self._context_mod_layers = nn.ModuleList() if self._use_context_mod \
            else None

        if self._use_context_mod:
            cm_layer_inds = []
            cm_shapes = [] # Output shape of all layers.
            if self._context_mod_inputs:
                cm_shapes.append([in_shape[2], *in_shape[:2]])
                # We reserve layer zero for input context-mod. Otherwise, there
                # is no layer zero.
                cm_layer_inds.append(0)

            layer_out_shapes = self._compute_layer_out_sizes()
            cm_shapes.extend(layer_out_shapes)
            # All layer indices `l` with `l mod 3 == 0` are context-mod layers.
            cm_layer_inds.extend(range(3, 3*len(layer_out_shapes)+1, 3))
            if self._no_last_layer_context_mod:
                cm_shapes = cm_shapes[:-1]
                cm_layer_inds = cm_layer_inds[:-1]
            if not self._context_mod_apply_pixel_wise:
                # Only scalar gain and shift per feature map!
                for i, s in enumerate(cm_shapes):
                    if len(s) == 3:
                        cm_shapes[i] = [s[0], 1, 1]

            self._add_context_mod_layers(cm_shapes, cm_layers=cm_layer_inds)

        ###############################
        ### Create batchnorm layers ###
        ###############################
        # We just use even numbers starting from 2 as layer indices for
        # batchnorm layers.
        if use_batch_norm:
            bn_sizes = []
            for i, s in enumerate(self._filter_sizes):
                if i == 0:
                    bn_sizes.append(s)
                else:
                    bn_sizes.extend([s] * (2*n))

            # All layer indices `l` with `l mod 3 == 2` are batchnorm layers.
            self._add_batchnorm_layers(bn_sizes, no_weights,
                bn_layers=list(range(2, 3*len(bn_sizes)+1, 3)),
                distill_bn_stats=distill_bn_stats,
                bn_track_stats=bn_track_stats)
            
            self.interval_bn_layers = [IntervalBatchNorm() for _ in range(len(bn_sizes))]

        self.relu = IntervalReLU()

        ######################################
        ### Create skip connection weights ###
        ######################################
        # We use 1x1 convolutional layers for residual blocks in case the
        # number of input and output feature maps disagrees. We also use 1x1
        # convolutions whenever a stride greater than 1 is applied. This is not
        # necessary in my opinion (as it adds extra weights that do not affect
        # the downsampling itself), but commonly done; for instance, in the
        # original PyTorch implementation.
        # Note, there may be maximally 3 1x1 layers added to the network.
        # Note, we use 1x1 conv layers without biases.
        skip_1x1_shapes = []
        self._group_has_1x1 = [False] * 3
        for i in range(1, 4):
            if self._filter_sizes[i-1] != self._filter_sizes[i] or \
                    self._strides[i] != 1:
                skip_1x1_shapes.append([self._filter_sizes[i],
                                        self._filter_sizes[i-1], 1, 1])
                self._group_has_1x1[i-1] = True

        for s in skip_1x1_shapes:
            if not no_weights:
                self._internal_params.append(nn.Parameter( \
                    torch.Tensor(*s), requires_grad=True))
                self._layer_weight_tensors.append(self._internal_params[-1])
                self._layer_bias_vectors.append(None)
                init_params(self._layer_weight_tensors[-1])
            else:
                self._hyper_shapes_learned.append(s)
                self._hyper_shapes_learned_ref.append(len(self.param_shapes))

            self._param_shapes.append(s)
            self._param_shapes_meta.append({
                'name': 'weight',
                'index': -1 if no_weights else \
                    len(self._internal_params)-1,
                'layer': -1
            })

        ############################################################
        ### Create convolutional layers and final linear weights ###
        ############################################################
        # Convolutional layers will get IDs `l` such that `l mod 3 == 1`.
        layer_id = 1
        for i in range(5):
            if i == 0: ### Fist layer.
                num = 1
                prev_fs = self._in_shape[2]
                curr_fs = self._filter_sizes[0]
            elif i == 4: ### Final fully-connected layer.
                num = 1
                curr_fs = num_classes
            else: # Group of residual blocks.
                num = 2 * n
                curr_fs = self._filter_sizes[i]

            for _ in range(num):
                if i == 4:
                    layer_shapes = [[curr_fs, prev_fs]]
                    if use_fc_bias:
                        layer_shapes.append([curr_fs])
                else:
                    layer_shapes = [[curr_fs, prev_fs, *self._kernel_size]]
                    if use_bias:
                        layer_shapes.append([curr_fs])

                for s in layer_shapes:
                    if not no_weights:
                        self._internal_params.append(nn.Parameter( \
                            torch.Tensor(*s), requires_grad=True))
                        if len(s) == 1:
                            self._layer_bias_vectors.append( \
                                self._internal_params[-1])
                        else:
                            self._layer_weight_tensors.append( \
                                self._internal_params[-1])
                    else:
                        self._hyper_shapes_learned.append(s)
                        self._hyper_shapes_learned_ref.append( \
                            len(self.param_shapes))

                    self._param_shapes.append(s)
                    self._param_shapes_meta.append({
                        'name': 'weight' if len(s) != 1 else 'bias',
                        'index': -1 if no_weights else \
                            len(self._internal_params)-1,
                        'layer': layer_id
                    })

                prev_fs = curr_fs
                layer_id += 3

                # Initialize_weights
                if not no_weights:
                    init_params(self._layer_weight_tensors[-1],
                        self._layer_bias_vectors[-1] \
                        if len(layer_shapes) == 2 else None)

        ###########################
        ### Print infos to user ###
        ###########################
        if verbose:
            if self._use_context_mod:
                cm_param_shapes = []
                for cm_layer in self.context_mod_layers:
                    cm_param_shapes.extend(cm_layer.param_shapes)
                cm_num_params = \
                    MainNetInterface.shapes_to_num_weights(cm_param_shapes)

            print('Creating a WideResnet "%s" with %d weights' \
                  % (str(self), self.num_params)
                  + (' (including %d weights associated with-' % cm_num_params
                     + 'context modulation)' if self._use_context_mod else '')
                  + '.'
                  + (' The network uses batchnorm.' if use_batch_norm  else '')
                  + (' The network uses dropout.' if dropout_rate != -1 \
                     else ''))

        self._is_properly_setup(check_has_bias=False)

    @property
    def has_bias(self):
        """Getter for read-only attribute :attr:`has_bias`."""
        if self._use_bias != self._use_fc_bias:
            raise RuntimeError('Attribute "has_bias" does not apply to a ' +
                               'network where there is a mixture of layers ' +
                               'with and without biases.')
        if self._use_bias and np.any(self._group_has_1x1):
            warn('The network contains skip connections which are realized ' +
                 'via 1x1 convolutional layers without biases. The attribute ' +
                 '"has_bias" ignores these layers. It\'s best to avoid using ' +
                 'this attribute for this reason.')
        return self._has_bias

    def forward(self, x: torch.Tensor, epsilon: float, weights: List[torch.Tensor]=None, 
                distilled_params: List=None, condition: int=None) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute the output :math:`y` of this network given the input
        :math:`x`.

        This method computes the forward pass, potentially using hypernetwork-generated
        weights and accounting for interval/uncertainty bounds via :math:`\\epsilon`.

        Args:
            x (torch.Tensor): The input image batch. Expected format is either 
                flattened ``HWC`` or unflattened ``CHW``, depending on the 
                ``chw_input_format`` constructor argument.
            epsilon (float): The uncertainty radius or perturbation bound for interval propagation.
            weights (optional, list or dict): Weights generated by a hypernetwork. These are only 
                used if the network was initialized with ``no_weights=True``.
            distilled_params (optional, list or dict): Parameters (e.g., Batch Norm statistics)
                used for distillation purposes.
            condition (optional, torch.Tensor): A conditioning vector used for context modulation
                if the network was configured for it.

        Returns:
            (torch.Tensor): The output of the network (logits).
        """
        if ((not self._use_context_mod and self._no_weights) or \
                (self._no_weights or self._context_mod_no_weights)) and \
                weights is None:
            raise Exception('Network was generated without weights. ' +
                            'Hence, "weights" option may not be None.')

        device = x.device
        
        ############################################
        ### Extract which weights should be used ###
        ############################################
        # I.e., are we using internally maintained weights or externally given
        # ones or are we even mixing between these groups.
        # FIXME code mostly copied from MLP forward method.
        n_cm = self._num_context_mod_shapes()

        if weights is None:
            weights = self.weights

            if self._use_context_mod:
                cm_weights = weights[:n_cm]
                int_weights = weights[n_cm:]
            else:
                cm_weights = None
                int_weights = weights
        else:
            int_weights = None
            cm_weights = None

            if isinstance(weights, dict):
                assert('internal_weights' in weights.keys() or \
                       'mod_weights' in weights.keys())
                if 'internal_weights' in weights.keys():
                    int_weights = weights['internal_weights']
                if 'mod_weights' in weights.keys():
                    cm_weights = weights['mod_weights']
            else:
                if self._use_context_mod and \
                        len(weights) == n_cm:
                    cm_weights = weights
                else:
                    assert len(weights) == len(self.param_shapes)
                    if self._use_context_mod:
                        cm_weights = weights[:n_cm]
                        int_weights = weights[n_cm:]
                    else:
                        int_weights = weights

            if self._use_context_mod and cm_weights is None:
                if self._context_mod_no_weights:
                    raise Exception('Network was generated without weights ' +
                        'for context-mod layers. Hence, they must be passed ' +
                        'via the "weights" option.')
                cm_weights = self.weights[:n_cm]
            if int_weights is None:
                if self._no_weights:
                    raise Exception('Network was generated without internal ' +
                        'weights. Hence, they must be passed via the ' +
                        '"weights" option.')
                if self._context_mod_no_weights:
                    int_weights = self.weights
                else:
                    int_weights = self.weights[n_cm:]

            # Note, context-mod weights might have different shapes, as they
            # may be parametrized on a per-sample basis.
            if self._use_context_mod:
                assert len(cm_weights) == self._num_context_mod_shapes()
            int_shapes = self.param_shapes[n_cm:]
            assert len(int_weights) == len(int_shapes)
            for i, s in enumerate(int_shapes):
                assert np.all(np.equal(s, list(int_weights[i].shape)))

        ### Split context-mod weights per context-mod layer.
        if cm_weights is not None:
            cm_weights_layer = []
            cm_start = 0
            for cm_layer in self.context_mod_layers:
                cm_end = cm_start + len(cm_layer.param_shapes)
                cm_weights_layer.append(cm_weights[cm_start:cm_end])
                cm_start = cm_end

        int_meta = self.param_shapes_meta[n_cm:]
        int_weights = list(int_weights)
        ### Split batchnorm weights layer-wise.
        if self._use_batch_norm:
            lbw = 2 * len(self.batchnorm_layers)

            bn_weights = int_weights[:lbw]
            int_weights = int_weights[lbw:]
            bn_meta = int_meta[:lbw]
            int_meta = int_meta[lbw:]

            bn_scales = []
            bn_shifts = []

            for i in range(len(self.batchnorm_layers)):
                assert bn_meta[2*i]['name'] == 'bn_scale'
                bn_scales.append(bn_weights[2*i])
                assert bn_meta[2*i+1]['name'] == 'bn_shift'
                bn_shifts.append(bn_weights[2*i+1])

        ### Split internal weights layer-wise.
        # Weights of skip connections.
        n_skip_1x1 = np.sum(self._group_has_1x1)
        skip_1x1_weights = [None] * 3
        for i in range(3):
            if self._group_has_1x1[i]:
                skip_1x1_weights[i] = int_weights.pop(0)
        int_meta = int_meta[n_skip_1x1:]

        # Weights/biases per layer.
        layer_weights = [None] * (6 * self._n + 2)
        layer_biases = [None] * (6 * self._n + 2)

        for i, meta in enumerate(int_meta):
            ltype = meta['name']
            # Recals, layer IDs for this type of layer are `l mod 3 == 1`.
            lid = (meta['layer'] - 1) // 3
            if ltype == 'weight':
                layer_weights[lid] = int_weights[i]
            else:
                assert ltype == 'bias'
                layer_biases[lid] = int_weights[i]

        #######################
        ### Parse condition ###
        #######################
        bn_cond = None
        cmod_cond = None

        if condition is not None:
            if isinstance(condition, dict):
                assert 'bn_stats_id' in condition.keys() or \
                       'cmod_ckpt_id' in condition.keys()
                if 'bn_stats_id' in condition.keys():
                    bn_cond = condition['bn_stats_id']
                if 'cmod_ckpt_id' in condition.keys():
                    cmod_cond = condition['cmod_ckpt_id']
            else:
                bn_cond = condition

        if cmod_cond is not None:
            # FIXME We always require context-mod weight above, but
            # we can't pass both (a condition and weights) to the
            # context-mod layers.
            # An unelegant solution would be, to just set all
            # context-mod weights to None.
            raise NotImplementedError('CM-conditions not implemented!')
            cm_weights_layer = [None] * len(cm_weights_layer)

        ######################################
        ### Select batchnorm running stats ###
        ######################################
        if self._use_batch_norm:
            nn = len(self._batchnorm_layers)
            running_means = [None] * nn
            running_vars = [None] * nn

        if distilled_params is not None:
            if not self._distill_bn_stats:
                raise ValueError('Argument "distilled_params" can only be ' +
                                 'provided if the return value of ' +
                                 'method "distillation_targets()" is not None.')
            shapes = self.hyper_shapes_distilled
            assert len(distilled_params) == len(shapes)
            for i, s in enumerate(shapes):
                assert np.all(np.equal(s, list(distilled_params[i].shape)))

            # Extract batchnorm stats from distilled_params
            for i in range(0, len(distilled_params), 2):
                running_means[i//2] = distilled_params[i]
                running_vars[i//2] = distilled_params[i+1]

        elif self._use_batch_norm and self._bn_track_stats and \
                bn_cond is None:
            for i, bn_layer in enumerate(self._batchnorm_layers):
                running_means[i], running_vars[i] = bn_layer.get_stats()

        ###########################
        ### Forward Computation ###
        ###########################
        cm_ind = 0
        bn_ind = 0
        layer_ind = 0

        ### Helper function to process convolutional layers.
        def conv_layer(self, mu: torch.Tensor, eps: torch.Tensor, stride: int, shortcut_mu: torch.Tensor = None, 
                   shortcut_eps: torch.Tensor = None, no_conv: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
            """Compute the output of a full convolutional layer within a residual block.

            This function includes batch normalization, context modulation, non-linearity, 
            and the final addition of a shortcut connection. It handles both the mean ($\mu$) 
            and the interval uncertainty ($\epsilon$) tensors.

            The order of operations is:
            $\text{Context-Mod (pre)} \rightarrow \text{Batch-Norm} \rightarrow \text{Non-Linearity} \rightarrow$
            $\text{Context-Mod (post)} \rightarrow \text{Conv-Layer} \rightarrow \text{Shortcut}$

            This method internally increments the indices ``layer_ind``, ``cm_ind``, and ``bn_ind``.

            Args:
                mu (torch.Tensor): The mean ($\mu$) component of the input activity interval.
                eps (torch.Tensor): The epsilon ($\epsilon$) component of the input activity interval (uncertainty radius).
                stride (int): The stride used for the convolutional layer (padding is automatically set to 1).
                shortcut_mu (optional, torch.Tensor): The mean ($\mu$) component of the skip connection tensor to be added *before* the non-linearity.
                shortcut_eps (optional, torch.Tensor): The epsilon ($\epsilon$) component of the skip connection tensor to be added *before* the non-linearity.
                no_conv (bool): If True, skips the application of the convolutional layer.

            Returns:
                Tuple[torch.Tensor, torch.Tensor]: A tuple containing the output mean ($\mu$) and epsilon ($\epsilon$) tensors of the layer.
            """
            nonlocal layer_ind, cm_ind, bn_ind

            # Context-dependent modulation (pre-activation).
            if self._use_context_mod and \
                    not self._context_mod_post_activation:
                mu = self._context_mod_layers[cm_ind].forward(mu,
                    weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond)
                cm_ind += 1

            # Batch-norm
            if self._use_batch_norm:
                mu, eps = self.interval_bn_layers[bn_ind].forward(
                    mu, eps,
                    weight=bn_scales[bn_ind],
                    bias=bn_shifts[bn_ind],
                    running_mean=running_means[bn_ind],
                    running_var=running_vars[bn_ind],
                    stats_id=bn_cond,
                    batch_norm_forward=self.batchnorm_layers[bn_ind].forward,
                    device=device
                )
                bn_ind += 1

            # Non-linearity
            mu, eps = self.relu.forward(mu, eps, device=device)

            # Context-dependent modulation (post-activation).
            if self._use_context_mod and self._context_mod_post_activation:
                mu = self._context_mod_layers[cm_ind].forward(mu,
                    weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond)
                cm_ind += 1

            if not no_conv:
                mu = F.conv2d(mu, layer_weights[layer_ind],
                    bias=layer_biases[layer_ind], stride=stride, padding=1)
                eps = F.conv2d(eps, layer_weights[layer_ind].abs(),
                    bias=None, stride=stride, padding=1)
                layer_ind += 1

            if shortcut_mu is not None and shortcut_eps is not None:
                mu += shortcut_mu
                eps += shortcut_eps

            return mu, eps
        
        if not self._chw_input_format:
            x = x.view(-1, *self._in_shape)
            x = x.permute(0, 3, 1, 2)
            epsilon = epsilon * torch.ones_like(x)
        mu, eps = x.to(device), epsilon.to(device)

        # Context-dependent modulation of inputs directly.
        if self._use_context_mod and self._context_mod_inputs:
            mu = self._context_mod_layers[cm_ind].forward(mu,
                weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond)
            cm_ind += 1

        ### Initial convolutional layer.
        mu = F.conv2d(mu, layer_weights[layer_ind], bias=layer_biases[layer_ind],
                     stride=self._strides[0], padding=1)
        eps = F.conv2d(
              input=eps,
              weight=layer_weights[layer_ind].abs(),
              bias=None,
              stride=self._strides[0],
              padding=1
          )
        layer_ind += 1

        ### Three groups, each containing n resnet blocks.
        for i in range(3):
            # Only the first layer in a group may be a strided convolution.
            stride = self._strides[i+1]
            # For each resnet block . A resnet block consists of 2 convolutional
            # layers.
            for j in range(self._n):
                shortcut_mu = mu
                shortcut_eps = eps
                if j == 0 and self._group_has_1x1[i]:
                    shortcut_mu = F.conv2d(mu, skip_1x1_weights[i], bias=None,
                                          stride=stride, padding=0)
                    shortcut_eps = F.conv2d(eps, skip_1x1_weights[i].abs(), bias=None,
                                          stride=stride, padding=0)

                mu, eps = conv_layer(
                    mu=mu,
                    eps=eps,
                    stride=stride,
                    shortcut_mu=None,
                    shortcut_eps=None
                )

                if self._dropout_rate != -1:
                    mu = self._dropout(mu)

                stride = 1

                mu, eps = conv_layer(
                    mu=mu,
                    eps=eps,
                    stride=stride,
                    shortcut_mu=shortcut_mu,
                    shortcut_eps=shortcut_eps
                )


        ### Final batchnorm + non-linearity.
        # Note, that the logical structure of a resnet block in a wide resnet
        # ends with the convolution. So we still need to apply the batchnorm
        # and non-linearity for the very last resnet block.
        mu, eps = conv_layer(
            mu=mu,
            eps=eps,
            stride=None,
            shortcut_mu=None,
            shortcut_eps=None,
            no_conv=True
        )

        ### Average pool all activities within a feature map.
        z_lower, z_upper = mu - eps, mu + eps
        z_lower = F.avg_pool2d(
            z_lower,
            [z_lower.size()[2], z_lower.size()[3]]
        )

        z_upper = F.avg_pool2d(
            z_upper,
            [z_upper.size()[2], z_upper.size()[3]]
        )

        mu, eps = (z_upper + z_lower) / 2.0, (z_upper - z_lower) / 2.0

        mu = mu.view(mu.size(0), -1)
        eps = eps.view(eps.size(0), -1)

        ### Apply final fully-connected layer and compute outputs.
        mu = F.linear(
            input=mu,
            weight=layer_weights[layer_ind],
            bias=layer_biases[layer_ind]
        )

        eps = F.linear(
            input=eps,
            weight=layer_weights[layer_ind].abs(),
            bias=None
        )

        # Context-dependent modulation in output layer.
        if self._use_context_mod and not self._no_last_layer_context_mod:
            mu = self._context_mod_layers[cm_ind].forward(mu,
                weights=cm_weights[2*cm_ind:2*cm_ind+2], ckpt_id=cmod_cond)

        return mu, eps


    def distillation_targets(self) -> Optional[List[torch.Tensor]]:
        """Targets to be distilled after training.

        Retrieves tensors intended for knowledge distillation, primarily the current
        batch statistics from Batch Normalization (BN) layers.

        This method returns the current BN batch statistics if both 
        ``distill_bn_stats`` and ``use_batch_norm`` were set to ``True`` in the constructor. 
        It returns ``None`` if there are no targets to distill.

        Returns:
            Optional[List[torch.Tensor]]: A list of target tensors corresponding to the 
            shapes specified in the attribute ``hyper_shapes_distilled``, or ``None`` if 
            distillation is disabled.
        """
        if self.hyper_shapes_distilled is None:
            return None

        ret = []
        for bn_layer in self._batchnorm_layers:
            ret.extend(bn_layer.get_stats())

        return ret


    def _compute_layer_out_sizes(self) -> List[List[int]]:
        """Compute the output shapes of all main layers in this network.

        This method calculates the output shape for every main layer, including the 
        final classification layer (which corresponds to the number of classes). It
        excludes any shapes related to skip connection layers.

        Returns:
            List[List[int]]: A list of shapes (lists of integers). The first entry 
            is the output shape of the first convolutional layer. The last entry 
            is the shape of the final output (logits).

        Note:
            Output shapes for convolutional layers adhere to PyTorch convention: 
            ``[C, H, W]``, where ``C`` is the channel dimension.
        """
        # FIXME Method has been copied and only slightly modified from class
        # `ResNet`.
        in_shape = self._in_shape
        fs = self._filter_sizes
        ks = self._kernel_size
        strides = self._strides
        pd = 1 # all paddings are 1.
        assert len(ks) == 2
        assert len(fs) == 4
        n = self._n

        # Note, `in_shape` is in Tensorflow layout.
        assert(len(in_shape) == 3)
        in_shape = [in_shape[2], *in_shape[:2]]

        ret = []

        C, H, W = in_shape

        # Recall the formular for convolutional layers:
        # W_new = (W - K + 2P) // S + 1

        # First conv layer.
        C = fs[0]
        H = (H - ks[0] + 2*pd) // strides[0] + 1
        W = (W - ks[1] + 2*pd) // strides[0] + 1
        ret.append([C, H, W])

        # First block (only first layer may have stride).
        C = fs[1]
        H = (H - ks[0] + 2*pd) // strides[1] + 1
        W = (W - ks[1] + 2*pd) // strides[1] + 1
        ret.extend([[C, H, W]] * (2*n))

        # Second block (only first layer may have stride).
        C = fs[2]
        H = (H - ks[0] + 2*pd) // strides[2] + 1
        W = (W - ks[1] + 2*pd) // strides[2] + 1
        ret.extend([[C, H, W]] * (2*n))

        # Third block (only first layer may have stride).
        C = fs[3]
        H = (H - ks[0] + 2*pd) // strides[3] + 1
        W = (W - ks[1] + 2*pd) // strides[3] + 1
        ret.extend([[C, H, W]] * (2*n))

        # Final fully-connected layer (after avg pooling), i.e., output size.
        ret.append([self._num_classes])

        assert len(ret) == 6*n + 2

        return ret

    def __str__(self) -> str:
        """Returns the canonical string representation of the WRN architecture.

        The string follows the notation: **WRN-d-k-B(3,3)**, where:
        - ``d`` is the total number of convolutional layers.
        - ``k`` is the widening factor.
        - ``B(3,3)`` indicates $3 \times 3$ kernel sizes.

        The total number of convolutional layers is calculated as $1 + 6n + \sum(\text{group\_has\_1x1})$.

        Returns:
            str: The WRN architecture string, e.g., 'WRN-28-10-B(3,3)'.
        """
        n_conv_layers = 1 +  6 * self._n + np.sum(self._group_has_1x1)
        # WRN-d-k-B(3,3)
        return 'WRN-%d-%d-B(3,3)' % (n_conv_layers, self._k)

    def get_output_weight_mask(self, out_inds: Optional[Union[List[int], torch.Tensor]] = None, 
                               device: Optional[Union[str, torch.device]] = None) -> List[torch.Tensor]:
        """Create a mask for selecting weights connected solely to certain output units.

        This method generates masks for the weight tensors of the network. The masks are 
        used to identify and isolate parameters that exclusively contribute to the specified 
        output units (e.g., for task-specific distillation or regularization).

        Args:
            out_inds (optional, list of int or torch.Tensor): The indices of the output units 
                for which the weight masks should be created. If ``None``, a full mask is often implied.
            device (optional, str or torch.device): The device on which the resulting mask tensors should be allocated.

        Returns:
            List[torch.Tensor]: A list of masks (binary tensors) corresponding to the network's 
            weight tensors, where `True` indicates a connection to the specified output units.
        """
        # Note: Super method `get_output_weight_mask` fails if `has_bias` fails.
        # Therefore, we simply replace `has_bias` with `_use_fc_bias`.

        if not (self.has_fc_out and self.mask_fc_out):
            raise NotImplementedError('Method not applicable for this ' +
                                      'network type.')

        ret = [None] * len(self.param_shapes)

        obias_ind = len(self.param_shapes)-1 if self._use_fc_bias else None
        oweights_ind = len(self.param_shapes)-2 if self._use_fc_bias \
            else len(self.param_shapes)-1

        # Bias weights for outputs.
        if obias_ind is not None:
            if out_inds is None:
                mask = torch.ones(*self.param_shapes[obias_ind],
                                  dtype=torch.bool)
            else:
                mask = torch.zeros(*self.param_shapes[obias_ind],
                                   dtype=torch.bool)
                mask[out_inds] = 1
            if device is not None:
                mask = mask.to(device)
            ret[obias_ind] = mask

        # Weights from weight matrix of output layer.
        if out_inds is None:
            mask = torch.ones(*self.param_shapes[oweights_ind],
                              dtype=torch.bool)
        else:
            mask = torch.zeros(*self.param_shapes[oweights_ind],
                               dtype=torch.bool)
            mask[out_inds, :] = 1
        if device is not None:
            mask = mask.to(device)
        ret[oweights_ind] = mask

        return ret



if __name__ == '__main__':
    pass
