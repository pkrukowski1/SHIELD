import torch
import torch.nn as nn
import torch.nn.functional as F
from warnings import warn
from typing import Tuple, List, Iterable

from hypnettorch.mnets.classifier_interface import Classifier
from hypnettorch.mnets.mnet_interface import MainNetInterface

from utils.interval_modules import *

class ReducedIntervalResNet18(Classifier):
    """
    Interval ResNet18 variant for ImageNet dataset variants with reduced parameters.

    This model propagates both the midpoint (mu) and radius (eps) of input intervals
    for robustness analysis. All weights are expected to be provided externally
    (e.g., by a hypernetwork), as indicated by `no_weights=True`.

    Args
        in_shape : tuple, optional
            Input shape as (H, W, C). Default: (64, 64, 3).
        num_classes : int, optional
            Number of output classes. Default: 100.
        use_bias : bool, optional
            If True, use biases in convolutional layers. Default: False.
        use_fc_bias : bool, optional
            If True, use bias in the final linear layer. Default: True.
        num_feature_maps : tuple, optional
            Number of filters per group. Default: (16, 32, 64, 128).
        blocks_per_group : tuple, optional
            Number of residual blocks per group. Default: (4, 4, 4, 4).
        projection_shortcut : bool, optional
            If True, use 1x1 convolution for skip connections. Default: True.
        bottleneck_blocks : bool, optional
            If True, use bottleneck blocks. Default: False.
        cutout_mod : bool, optional
            If True, apply cutout augmentation. Default: False.
        no_weights : bool, optional
            If True, weights are not stored in the model and must be provided externally.
            Default: True.
        use_batch_norm : bool, optional
            If True, apply batch normalization. Default: True.
        bn_track_stats : bool, optional
            If True, track running statistics in batch normalization. Default: True.
        distill_bn_stats : bool, optional
            If True, allow distillation of batch norm statistics. Default: False.
        chw_input_format : bool, optional
            If True, input is expected in (C, H, W) format. Default: False.
        verbose : bool, optional
            If True, print initialization info. Default: True.
        device : str, optional
            Device to use ("cpu" or "cuda"). Default: "cpu".
        **kwargs
            Additional arguments for context modulation (e.g., use_context_mod, etc.).

    Attributes
        _filter_sizes : list
            List of filter counts for each group, including initial conv.
        _num_main_conv_layers : int
            Total number of main convolutional layers.
        _num_non_ident_skips : int
            Number of skip connections using 1x1 convolutions.
        initial_conv : IntervalConv2d
            Initial convolutional layer.
        initial_pool : IntervalAvgPool2d
            Initial average pooling layer.
        module_convs : list
            List of convolutional layers for each residual block.
        skip_convs : list
            List of 1x1 convolutional layers for skip connections.
        interval_bn_layers : list
            List of interval batch normalization layers.
        relu : IntervalReLU
            Interval ReLU activation.
        final_pool : IntervalAvgPool2d
            Final average pooling layer.
        flatten : IntervalFlatten
            Flattening layer before the classifier.
        linear : IntervalLinear
            Final linear classification layer.
        _hyper_shapes_learned : list
            List of parameter shapes for hypernetwork-generated weights.
        _param_shapes : list
            List of all parameter shapes.
    """
    def __init__(
        self,
        in_shape: Tuple[int,int,int]=(64, 64, 3),
        num_classes: int=100,
        use_bias: bool=False,
        use_fc_bias: bool=True,
        num_feature_maps: Tuple[int,int,int,int]=(16, 32, 64, 128),
        blocks_per_group: Tuple[int,int,int,int]=(4, 4, 4, 4),
        projection_shortcut: bool=True,
        bottleneck_blocks: bool=False,
        cutout_mod: bool=False,
        no_weights: bool=True,
        use_batch_norm: bool=True,
        bn_track_stats: bool=True,
        distill_bn_stats: bool=False,
        chw_input_format: bool=False,
        verbose: bool=True,
        device: str="cpu",
        **kwargs
    ) -> None:
        super(ReducedIntervalResNet18, self).__init__(num_classes, verbose)
        assert no_weights, "Learnable parameters are only generated by a hypernetwork"

        # Parse context-mod arguments
        rem_kwargs = MainNetInterface._parse_context_mod_args(kwargs)
        if "context_mod_apply_pixel_wise" in rem_kwargs:
            rem_kwargs.remove("context_mod_apply_pixel_wise")
        if len(rem_kwargs) > 0:
            raise ValueError(f"Unknown keyword arguments: {rem_kwargs}")
        if "context_mod_apply_pixel_wise" not in kwargs:
            kwargs["context_mod_apply_pixel_wise"] = False

        self._use_context_mod = kwargs.get("use_context_mod", False)
        self._context_mod_inputs = kwargs.get("context_mod_inputs", False)
        self._no_last_layer_context_mod = kwargs.get("no_last_layer_context_mod", False)
        self._context_mod_no_weights = kwargs.get("context_mod_no_weights", False)
        self._context_mod_post_activation = kwargs.get("context_mod_post_activation", False)
        self._context_mod_gain_offset = kwargs.get("context_mod_gain_offset", False)
        self._context_mod_gain_softplus = kwargs.get("context_mod_gain_softplus", False)
        self._context_mod_apply_pixel_wise = kwargs["context_mod_apply_pixel_wise"]
        self.device = device

        # Validate and set arguments
        self._in_shape = in_shape
        self._projection_shortcut = projection_shortcut
        self._bottleneck_blocks = bottleneck_blocks
        self._cutout_mod = cutout_mod
        self._use_bias = use_bias
        self._use_fc_bias = use_fc_bias
        self._no_weights = no_weights
        self._use_batch_norm = use_batch_norm
        self._bn_track_stats = bn_track_stats
        self._distill_bn_stats = distill_bn_stats and use_batch_norm
        self._chw_input_format = chw_input_format

        if len(blocks_per_group) != 4:
            raise ValueError("blocks_per_group must be a tuple of 4 integers")
        self._num_blocks = blocks_per_group
        if len(num_feature_maps) != 4:
            raise ValueError("num_feature_maps must be a tuple of 4 integers")
        self._filter_sizes = [16] + list(num_feature_maps)

        self._init_kernel_size = (3, 3)
        self._init_padding = 1
        self._init_stride = 2

        self._num_non_ident_skips = 0
        self._group_has_1x1 = [False] * 4
        if projection_shortcut:
            for i in range(4):
                if i == 0 or self._filter_sizes[i] != self._filter_sizes[i + 1]:
                    self._group_has_1x1[i] = True
                    self._num_non_ident_skips += 1

        self._num_main_conv_layers = 1 + sum(self._num_blocks) * 2

        self._has_bias = use_bias if use_bias == use_fc_bias else False
        self._has_fc_out = True
        self._mask_fc_out = True
        self._has_linear_out = True

        self._param_shapes = []
        self._param_shapes_meta = []
        self._internal_params = None
        self._hyper_shapes_learned = []
        self._hyper_shapes_learned_ref = []
        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()

        # Context modulation layers
        self._context_mod_layers = [] if self._use_context_mod else None
        if self._use_context_mod:
            cm_layer_inds = []
            cm_shapes = []
            if self._context_mod_inputs:
                cm_shapes.append([in_shape[2], *in_shape[:2]])
                cm_layer_inds.append(0)
            layer_out_shapes = self._compute_layer_out_sizes()
            cm_shapes.extend(layer_out_shapes)
            cm_layer_inds.extend(range(3, 3 * len(layer_out_shapes) + 1, 3))
            if self._no_last_layer_context_mod:
                cm_shapes = cm_shapes[:-1]
                cm_layer_inds = cm_layer_inds[:-1]
            if not self._context_mod_apply_pixel_wise:
                for i, s in enumerate(cm_shapes):
                    if len(s) == 3:
                        cm_shapes[i] = [s[0], 1, 1]
            self._add_context_mod_layers(cm_shapes, cm_layers=cm_layer_inds)

        # Batchnorm layers
        if use_batch_norm:
            bn_sizes = [self._filter_sizes[0]]
            for i in range(4):
                for _ in range(self._num_blocks[i]):
                    bn_sizes.extend([self._filter_sizes[i + 1]] * 2)
            bn_layers = list(range(2, 3 * len(bn_sizes) + 1, 3))
            if projection_shortcut:
                bn_layer_ind_skip = 3 * (self._num_main_conv_layers + 1) + 2
                for i in range(4):
                    if self._group_has_1x1[i]:
                        bn_sizes.append(self._filter_sizes[i + 1])
                        bn_layers.append(bn_layer_ind_skip)
                        bn_layer_ind_skip += 3
            self._add_batchnorm_layers(
                bn_sizes,
                no_weights,
                bn_layers=bn_layers,
                distill_bn_stats=distill_bn_stats,
                bn_track_stats=bn_track_stats,
            )

        # Initialize interval layers
        self.initial_conv = IntervalConv2d(
            in_channels=in_shape[2],
            out_channels=self._filter_sizes[0],
            kernel_size=self._init_kernel_size,
            stride=self._init_stride,
            padding=self._init_padding,
            device=device
        )
        self.initial_pool = IntervalAvgPool2d(kernel_size=2, stride=2)

        self.module_convs = []
        prev_fs = in_shape[2]
        for i in range(4):
            curr_fs = self._filter_sizes[i + 1]
            for j in range(self._num_blocks[i]):
                stride = 2 if j == 0 and i > 0 else 1
                self.module_convs.append(
                    IntervalConv2d(
                        in_channels=prev_fs if j == 0 else curr_fs,
                        out_channels=curr_fs,
                        kernel_size=(3, 3),
                        stride=stride,
                        padding=1,
                        device=device
                    )
                )
                self.module_convs.append(
                    IntervalConv2d(
                        in_channels=curr_fs,
                        out_channels=curr_fs,
                        kernel_size=(3, 3),
                        stride=1,
                        padding=1,
                        device=device
                    )
                )
                prev_fs = curr_fs

        self.skip_convs = []
        if projection_shortcut:
            n_in = self._filter_sizes[0]
            for i in range(4):
                if not self._group_has_1x1[i]:
                    self.skip_convs.append(None)
                    continue
                n_out = self._filter_sizes[i + 1]
                stride = 2 if i > 0 else 1
                self.skip_convs.append(
                    IntervalConv2d(
                        in_channels=n_in,
                        out_channels=n_out,
                        kernel_size=(1, 1),
                        stride=stride,
                        padding=0,
                        device=device
                    )
                )
                n_in = n_out

        self.interval_bn_layers = [IntervalBatchNorm() for _ in range(len(bn_sizes))]
        self.relu = IntervalReLU()
        self.final_pool = IntervalAvgPool2d(kernel_size=2, stride=2)
        self.flatten = IntervalFlatten()
        fc_in_size = self._filter_sizes[-1]  # Force 128 to match architecture
        self.linear = IntervalLinear(fc_in_size, num_classes, device=device)

        # Parameter shapes for hypernetwork
        if projection_shortcut:
            layer_ind_skip = 3 * (self._num_main_conv_layers + 1) + 1
            n_in = self._filter_sizes[0]
            for i in range(4):
                if not self._group_has_1x1[i]:
                    continue
                n_out = self._filter_sizes[i + 1]
                skip_1x1_shape = [n_out, n_in, 1, 1]
                self._hyper_shapes_learned.append(skip_1x1_shape)
                self._hyper_shapes_learned_ref.append(len(self._param_shapes))
                self._param_shapes.append(skip_1x1_shape)
                self._param_shapes_meta.append(
                    {"name": "weight", "index": -1, "layer": layer_ind_skip}
                )
                layer_ind_skip += 3
                n_in = n_out

        layer_id = 1
        prev_fs = self._in_shape[2]
        for i in range(5):
            if i == 0:
                num = 1
                curr_fs = self._filter_sizes[0]
                kernel_size = self._init_kernel_size
                stride = self._init_stride
                padding = self._init_padding
            else:
                num = self._num_blocks[i - 1] * 2
                curr_fs = self._filter_sizes[i]
                kernel_size = (3, 3)
                stride = 1
                padding = 1

            for n in range(num):
                if i > 0 and n % 2 == 0:
                    stride = 2 if i > 0 else 1
                else:
                    stride = 1
                layer_shapes = [[curr_fs, prev_fs, *kernel_size]]
                if use_bias:
                    layer_shapes.append([curr_fs])
                for s in layer_shapes:
                    self._hyper_shapes_learned.append(s)
                    self._hyper_shapes_learned_ref.append(len(self._param_shapes))
                    self._param_shapes.append(s)
                    self._param_shapes_meta.append(
                        {
                            "name": "weight" if len(s) != 1 else "bias",
                            "index": -1,
                            "layer": layer_id,
                        }
                    )
                layer_id += 3
                prev_fs = curr_fs

        layer_shapes = [[num_classes, fc_in_size]]
        if use_fc_bias:
            layer_shapes.append([num_classes])
        for s in layer_shapes:
            self._hyper_shapes_learned.append(s)
            self._hyper_shapes_learned_ref.append(len(self._param_shapes))
            self._param_shapes.append(s)
            self._param_shapes_meta.append(
                {
                    "name": "weight" if len(s) != 1 else "bias",
                    "index": -1,
                    "layer": layer_id,
                }
            )

        if verbose:
            print(
                f'Creating an interval "{self}" with {len(self._hyper_shapes_learned)} weight tensors'
                + (" with batchnorm." if use_batch_norm else ".")
            )
        self._is_properly_setup(check_has_bias=False)

    def __str__(self):
        return "ReducedIntervalResNet18"

    @property
    def has_bias(self):
        """
        Determines whether the model uses bias parameters in its layers.
        Returns:
            bool: True if bias is used in the model's layers, False otherwise.
        Raises:
            RuntimeError: If the general bias usage (`use_bias`) and the fully connected layer bias usage (`use_fc_bias`) differ,
                          as this method does not support that configuration.
        Warns:
            UserWarning: If bias is used and projection shortcuts are enabled, warns that skip connections use 1x1 convolutions
                         without biases and that this method ignores those cases.
        Note:
            This method assumes that the bias configuration is consistent across the model unless otherwise specified.
        """

        if self._use_bias != self._use_fc_bias:
            raise RuntimeError("has_bias does not apply when use_bias and use_fc_bias differ.")
        if self._use_bias and self._projection_shortcut:
            warn("Skip connections use 1x1 convs without biases. has_bias ignores these.")
        return self._has_bias

    def forward(self, x: torch.Tensor, epsilon: float, weights: List[torch.Tensor],
                distilled_params=None, condition: int=None, device: str="cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute the output midpoint (mu) and radius (eps) of this network given the input.

        Parameters
        ----------
        x : torch.Tensor
            Input image tensor. Shape: (N, H, W, C) if chw_input_format is False, else (N, C, H, W).
        epsilon : float
            Radius tensor for the input interval. Should be broadcastable to x.
        weights : list or dict
            List or dict of network weights (from a hypernetwork).
        distilled_params : list, optional
            Optional distilled batch norm statistics.
        condition : int or dict, optional
            Optional condition for batch norm or context modulation.
        device : str, optional
            Device to run the computation on.

        Returns
        -------
        tuple of torch.Tensor
            (mu, eps): Output midpoint and radius tensors.
        """
        if (
            (not self._use_context_mod and self._no_weights)
            or (self._no_weights or self._context_mod_no_weights)
        ) and weights is None:
            raise Exception("Weights must be provided for no_weights networks.")

        assert weights is not None, "Weights must not be None"

        n_cm = self._num_context_mod_shapes()
        int_weights, cm_weights = None, None
        if isinstance(weights, dict):
            int_weights = weights.get("internal_weights")
            cm_weights = weights.get("mod_weights")
        else:
            cm_weights = weights[:n_cm] if self._use_context_mod else None
            int_weights = weights[n_cm:] if self._use_context_mod else weights
        if cm_weights is None and self._use_context_mod:
            if self._context_mod_no_weights:
                raise Exception("Context-mod weights required.")
            cm_weights = []  # Fallback
        if int_weights is None:
            if self._no_weights:
                raise Exception("Internal weights required.")
            int_weights = []  # Fallback
        if self._use_context_mod:
            assert len(cm_weights) == n_cm
        int_shapes = self.param_shapes[n_cm:]
        assert len(int_weights) == len(int_shapes), f"Expected {len(int_shapes)} weights, got {len(int_weights)}"
        for i, s in enumerate(int_shapes):
            assert list(int_weights[i].shape) == s, f"Shape mismatch at index {i}: expected {s}, got {int_weights[i].shape}"

        cm_weights_layer = []
        if cm_weights is not None:
            cm_start = 0
            for cm_layer in self.context_mod_layers:
                cm_end = cm_start + len(cm_layer.param_shapes)
                cm_weights_layer.append(cm_weights[cm_start:cm_end])
                cm_start = cm_end

        int_meta = self.param_shapes_meta[n_cm:]
        int_weights = list(int_weights)

        if self._use_batch_norm:
            lbw = 2 * len(self.batchnorm_layers)
            bn_weights = int_weights[:lbw]
            int_weights = int_weights[lbw:]
            bn_meta = int_meta[:lbw]
            int_meta = int_meta[lbw:]
            bn_scales = [bn_weights[2 * i] for i in range(len(self.batchnorm_layers))]
            bn_shifts = [bn_weights[2 * i + 1] for i in range(len(self.batchnorm_layers))]
        else:
            bn_scales, bn_shifts = [], []

        n_skip_1x1 = sum(self._group_has_1x1)
        skip_1x1_weights = [None] * 4
        for i in range(4):
            if self._group_has_1x1[i]:
                skip_1x1_weights[i] = int_weights.pop(0)
        int_meta = int_meta[n_skip_1x1:]

        layer_weights = [None] * (self._num_main_conv_layers + 1)
        layer_biases = [None] * (self._num_main_conv_layers + 1)
        for i, meta in enumerate(int_meta):
            ltype = meta["name"]
            lid = (meta["layer"] - 1) // 3
            if ltype == "weight":
                layer_weights[lid] = int_weights[i]
            else:
                layer_biases[lid] = int_weights[i]

        bn_cond = None
        cmod_cond = None
        if condition is not None:
            if isinstance(condition, dict):
                bn_cond = condition.get("bn_stats_id")
                cmod_cond = condition.get("cmod_ckpt_id")
            else:
                bn_cond = condition
        if cmod_cond is not None:
            raise NotImplementedError("Context-mod conditions not supported.")

        if self._use_batch_norm:
            nn = len(self.batchnorm_layers)
            running_means = [None] * nn
            running_vars = [None] * nn
            if distilled_params is not None:
                if not self._distill_bn_stats:
                    raise ValueError("distilled_params requires distill_bn_stats.")
                for i in range(0, len(distilled_params), 2):
                    running_means[i // 2] = distilled_params[i]
                    running_vars[i // 2] = distilled_params[i + 1]
            elif self._bn_track_stats and bn_cond is None:
                for i, bn_layer in enumerate(self.batchnorm_layers):
                    running_means[i], running_vars[i] = bn_layer.get_stats()
        else:
            running_means, running_vars = [], []

        cm_ind = 0
        bn_ind = 0
        layer_ind = 0
        conv_ind = 0

        def conv_layer(mu: torch.Tensor, eps: torch.Tensor, stride: int=1, padding: int=1, shortcut_mu: torch.Tensor=None,
                       shortcut_eps: torch.Tensor=None, no_conv: bool=False, conv_layer: torch.nn.Module=None) -> Tuple[torch.Tensor,torch.Tensor]:
            """
            Apply a convolutional layer with optional context modulation, batch normalization, shortcut connection, and activation.
            Args:
                mu (torch.Tensor): The mean tensor input to the layer.
                eps (torch.Tensor): The epsilon (uncertainty) tensor input to the layer.
                stride (int, optional): Stride for the convolution. Defaults to 1.
                padding (int, optional): Padding for the convolution. Defaults to 1.
                shortcut_mu (torch.Tensor, optional): Mean tensor from a shortcut connection. Defaults to None.
                shortcut_eps (torch.Tensor, optional): Epsilon tensor from a shortcut connection. Defaults to None.
                no_conv (bool, optional): If True, skip the convolution operation. Defaults to False.
                conv_layer (nn.Module, optional): The convolutional layer to use. Required if no_conv is False.
            Returns:
                Tuple[torch.Tensor, torch.Tensor]: The output mean and epsilon tensors after applying the layer operations.
            Notes:
                - Applies convolution (unless `no_conv` is True), context modulation (pre- and/or post-activation), batch normalization,
                    shortcut addition, and ReLU activation in sequence.
                - Uses nonlocal variables for layer indices and weights.
                - Assumes presence of context modulation and batch normalization layers if enabled in the parent class.
            """
            
            nonlocal layer_ind, cm_ind, bn_ind, conv_ind
            if not no_conv:
                mu, eps = conv_layer.forward(
                    mu, eps,
                    weight=layer_weights[layer_ind],
                    bias=layer_biases[layer_ind],
                    device=device
                )
                layer_ind += 1
            if self._use_context_mod and not self._context_mod_post_activation:
                mu = self._context_mod_layers[cm_ind].forward(
                    mu, weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond
                )
                cm_ind += 1
            if self._use_batch_norm:
                mu, eps = self.interval_bn_layers[bn_ind].forward(
                    mu, eps,
                    weight=bn_scales[bn_ind],
                    bias=bn_shifts[bn_ind],
                    running_mean=running_means[bn_ind],
                    running_var=running_vars[bn_ind],
                    stats_id=bn_cond,
                    batch_norm_forward=self.batchnorm_layers[bn_ind].forward,
                    device=device
                )
                bn_ind += 1
            if shortcut_mu is not None and shortcut_eps is not None:
                mu = mu + shortcut_mu
                eps = eps + shortcut_eps
            mu, eps = self.relu.forward(mu, eps, device=device)
            if self._use_context_mod and self._context_mod_post_activation:
                mu = self._context_mod_layers[cm_ind].forward(
                    mu, weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond
                )
                cm_ind += 1
            return mu, eps

        # Handle input format
        if not self._chw_input_format:
            x = x.view(-1, *self._in_shape)
            x = x.permute(0, 3, 1, 2)
            epsilon = epsilon * torch.ones_like(x)
        mu, eps = x.to(device), epsilon.to(device)

        if self._use_context_mod and self._context_mod_inputs:
            mu = self._context_mod_layers[cm_ind].forward(
                mu, weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond
            )
            cm_ind += 1

        # Initial conv and pool
        mu, eps = conv_layer(mu, eps, conv_layer=self.initial_conv)
        mu, eps = self.initial_pool.forward(mu, eps, device=device)

        # Modules
        fs_prev = self._filter_sizes[0]
        for i in range(4):
            fs_curr = self._filter_sizes[i + 1]
            for j in range(self._num_blocks[i]):
                shortcut_mu, shortcut_eps = mu, eps
                stride = 2 if j == 0 and i > 0 else 1
                if j == 0 and (stride != 1 or fs_prev != fs_curr):
                    if self._projection_shortcut and self._group_has_1x1[i]:
                        shortcut_mu, shortcut_eps = self.skip_convs[i].forward(
                            mu, eps,
                            weight=skip_1x1_weights[i],
                            bias=None,
                            device=device
                        )
                        if self._use_batch_norm:
                            bn_short = len(self.batchnorm_layers) - sum(self._group_has_1x1) + i
                            shortcut_mu, shortcut_eps = self.interval_bn_layers[bn_short].forward(
                                shortcut_mu, shortcut_eps,
                                weight=bn_scales[bn_short],
                                bias=bn_shifts[bn_short],
                                running_mean=running_means[bn_short],
                                running_var=running_vars[bn_short],
                                stats_id=bn_cond,
                                batch_norm_forward=self.batchnorm_layers[bn_short].forward,
                                device=device
                            )
                    else:
                        pad_left = (fs_curr - fs_prev) // 2
                        pad_right = (fs_curr - fs_prev + 1) // 2
                        if stride == 2:
                            shortcut_mu = shortcut_mu[:, :, ::2, ::2]
                            shortcut_eps = shortcut_eps[:, :, ::2, ::2]
                        shortcut_mu = F.pad(
                            shortcut_mu, (0, 0, 0, 0, pad_left, pad_right), "constant", 0
                        )
                        shortcut_eps = F.pad(
                            shortcut_eps, (0, 0, 0, 0, pad_left, pad_right), "constant", 0
                        )
                # First conv in block
                mu, eps = conv_layer(
                    mu, eps,
                    stride=stride,
                    padding=1,
                    conv_layer=self.module_convs[conv_ind]
                )
                conv_ind += 1
                # Second conv in block
                mu, eps = conv_layer(
                    mu, eps,
                    stride=1,
                    padding=1,
                    shortcut_mu=shortcut_mu,
                    shortcut_eps=shortcut_eps,
                    conv_layer=self.module_convs[conv_ind]
                )
                conv_ind += 1
            fs_prev = fs_curr

        # Final layers
        mu, eps = self.final_pool.forward(mu, eps, device=device)
        mu, eps = self.flatten.forward(mu, eps, device=device)
        mu, eps = self.linear.forward(
            mu, eps,
            weight=layer_weights[layer_ind],
            bias=layer_biases[layer_ind],
            device=device
        )

        if self._use_context_mod and not self._no_last_layer_context_mod:
            mu = self._context_mod_layers[cm_ind].forward(
                mu, weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond
            )

        return mu, eps

    def distillation_targets(self) -> List:
        """
        Returns the distillation targets for the model, which are the statistics (e.g., mean and variance)
        from each batch normalization layer. If `hyper_shapes_distilled` is None, returns None.
        Returns:
            list or None: A list containing the statistics from each batch normalization layer,
            or None if distillation targets are not available.
        """

        if self.hyper_shapes_distilled is None:
            return None
        ret = []
        for bn_layer in self._batchnorm_layers:
            ret.extend(bn_layer.get_stats())
        return ret

    def _compute_layer_out_sizes(self) -> List:
        """
        Computes and returns the output sizes of each layer in the reduced ResNet-18 architecture.
        The method calculates the spatial dimensions (height and width) and channel depth for each layer,
        starting from the input shape and applying the sequence of convolutional and pooling operations
        as defined by the network's configuration. The output is a list where each element corresponds
        to the output shape of a layer in the form [channels, height, width], with the final element
        representing the number of output classes.
        Returns:
            list: A list of output shapes for each layer in the network, where each shape is represented
                  as [channels, height, width] for convolutional layers and [num_classes] for the final output.
        """

        in_shape = [self._in_shape[2], *self._in_shape[:2]]
        fs = self._filter_sizes
        ret = []
        C, H, W = in_shape

        C = fs[0]
        H = (H - 3 + 2 * 1) // 2 + 1
        W = (W - 3 + 2 * 1) // 2 + 1
        H = H // 2
        W = W // 2
        ret.append([C, H, W])

        def add_block(H: int, W: int, C: int, stride: int) -> Tuple[int,int,int]:
            """
            Adds two convolutional blocks to the network, updating the spatial dimensions and channel count.
            Parameters:
                H (int): The current height of the feature map.
                W (int): The current width of the feature map.
                C (int): The number of output channels for the block.
                stride (int): The stride to be used for the first convolution in the block.
            Returns:
                tuple: Updated (H, W, C) after applying two convolutional layers.
            Side Effects:
                Appends the output shape [C, H, W] after each convolutional layer to the global 'ret' list.
            """

            H = (H - 3 + 2 * 1) // stride + 1
            W = (W - 3 + 2 * 1) // stride + 1
            ret.append([C, H, W])
            H = (H - 3 + 2 * 1) // 1 + 1
            W = (W - 3 + 2 * 1) // 1 + 1
            ret.append([C, H, W])
            return H, W, C

        for i in range(4):
            for b in range(self._num_blocks[i]):
                stride = 2 if b == 0 and i > 0 else 1
                H, W, C = add_block(H, W, fs[i + 1], stride)

        H = H // 2
        W = W // 2
        ret.append([fs[-1], H, W])
        ret.append([self._num_classes])
        return ret

    def get_output_weight_mask(self, out_inds: Iterable[int]=None, device: str=None) -> torch.Tensor:
        """
        Returns a mask for the output weights of the network.
        This method generates a mask for the output weights, which can be used to select or modify specific output neurons.
        It delegates the actual mask generation to the superclass implementation.
        Args:
            out_inds (Optional[Iterable[int]]): Indices of the output neurons to include in the mask. If None, all outputs are included.
            device (Optional[torch.device or str]): The device on which to create the mask tensor. If None, uses the default device.
        Returns:
            torch.Tensor: A mask tensor for the output weights, with the same shape as the output layer's weights.
        """

        return super().get_output_weight_mask(out_inds=out_inds, device=device)