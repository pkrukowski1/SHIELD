import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from warnings import warn
from typing import Tuple

from hypnettorch.mnets.classifier_interface import Classifier
from hypnettorch.mnets.mnet_interface import MainNetInterface
from utils.interval_modules import *

class IntervalResNet18(Classifier):
    """
    Implements an interval ResNet18 variant for Split-miniImageNet with reduced parameters.
    Propagates midpoint (mu) and radius (eps) for robustness analysis. Weights are generated
    by a hypernetwork (no_weights=True), with total parameters (architecture + hypernetwork)
    below 9,298,380, matching the original ResNet18.

    Parameters:
    -----------
        in_shape (tuple): Input shape (H, W, C), default (64, 64, 3).
        num_classes (int): Number of output classes, default 100.
        use_bias (bool): If True, use biases in conv layers, default False.
        use_fc_bias (bool): If True, use bias in final linear layer, default True.
        num_feature_maps (tuple): Filters per module, default (16, 32, 64, 128).
        blocks_per_group (tuple): Blocks per module, default (4, 4, 4, 4).
        projection_shortcut (bool): Use 1x1 conv for skip connections, default True.
        bottleneck_blocks (bool): Use bottleneck blocks, default False.
        cutout_mod (bool): Apply cutout, default False.
        no_weights (bool): Weights from hypernetwork, default True.
        use_batch_norm (bool): Apply batch norm, default True.
        bn_track_stats (bool): Track BN stats, default True.
        distill_bn_stats (bool): Allow BN stats distillation, default False.
        chw_input_format (bool): Expect CHW input, default False.
        verbose (bool): Print init info, default True.
        mode (str): Dataset mode ("default", "tiny", "cifar", "cub"), default "default".
        device (str): Device ("cpu" or "cuda"), default "cpu".
        **kwargs: Context modulation args (e.g., use_context_mod).

    Attributes:
    -----------
        _filter_sizes (list): Filter counts [16, 16, 32, 64, 128].
        _num_main_conv_layers (int): Total conv layers (33).
        _num_non_ident_skips (int): Non-identity skip connections (3).
        initial_conv (IntervalConv2d): Initial conv layer.
        initial_pool (IntervalAvgPool2d): First (2x2) avg pool.
        module_convs (list): Conv layers for modules.
        skip_convs (list): 1x1 convs for skip connections.
        interval_bn_layers (list): Interval BN layers.
        relu (IntervalReLU): ReLU activation.
        final_pool (IntervalAvgPool2d): Final (2x2) avg pool.
        flatten (IntervalFlatten): Flattens features.
        linear (IntervalLinear): Final classification layer.
        _hyper_shapes_learned (list): Weight shapes for hypernetwork.
        _param_shapes (list): Parameter shapes.
    """
    def __init__(
        self,
        in_shape=(64, 64, 3),
        num_classes=100,
        use_bias=False,
        use_fc_bias=True,
        num_feature_maps=(16, 32, 64, 128),
        blocks_per_group=(4, 4, 4, 4),
        projection_shortcut=True,
        bottleneck_blocks=False,
        cutout_mod=False,
        no_weights=True,
        use_batch_norm=True,
        bn_track_stats=True,
        distill_bn_stats=False,
        chw_input_format=False,
        verbose=True,
        mode="default",
        device="cpu",
        **kwargs
    ):
        super(IntervalResNet18, self).__init__(num_classes, verbose)
        assert no_weights, "Learnable parameters are only generated by a hypernetwork"

        # Parse context-mod arguments
        rem_kwargs = MainNetInterface._parse_context_mod_args(kwargs)
        if "context_mod_apply_pixel_wise" in rem_kwargs:
            rem_kwargs.remove("context_mod_apply_pixel_wise")
        if len(rem_kwargs) > 0:
            raise ValueError(f"Unknown keyword arguments: {rem_kwargs}")
        if "context_mod_apply_pixel_wise" not in kwargs:
            kwargs["context_mod_apply_pixel_wise"] = False

        self._use_context_mod = kwargs.get("use_context_mod", False)
        self._context_mod_inputs = kwargs.get("context_mod_inputs", False)
        self._no_last_layer_context_mod = kwargs.get("no_last_layer_context_mod", False)
        self._context_mod_no_weights = kwargs.get("context_mod_no_weights", False)
        self._context_mod_post_activation = kwargs.get("context_mod_post_activation", False)
        self._context_mod_gain_offset = kwargs.get("context_mod_gain_offset", False)
        self._context_mod_gain_softplus = kwargs.get("context_mod_gain_softplus", False)
        self._context_mod_apply_pixel_wise = kwargs["context_mod_apply_pixel_wise"]
        self.device = device

        # Validate and set arguments
        self._in_shape = in_shape
        self._projection_shortcut = projection_shortcut
        self._bottleneck_blocks = bottleneck_blocks
        self._cutout_mod = cutout_mod
        self._use_bias = use_bias
        self._use_fc_bias = use_fc_bias
        self._no_weights = no_weights
        self._use_batch_norm = use_batch_norm
        self._bn_track_stats = bn_track_stats
        self._distill_bn_stats = distill_bn_stats and use_batch_norm
        self._chw_input_format = chw_input_format

        if len(blocks_per_group) != 4:
            raise ValueError("blocks_per_group must be a tuple of 4 integers")
        self._num_blocks = blocks_per_group
        if len(num_feature_maps) != 4:
            raise ValueError("num_feature_maps must be a tuple of 4 integers")
        self._filter_sizes = [16] + list(num_feature_maps)

        self._init_kernel_size = (3, 3)
        self._init_padding = 1
        self._init_stride = 2

        self._num_non_ident_skips = 0
        self._group_has_1x1 = [False] * 4
        if projection_shortcut:
            for i in range(4):
                if i == 0 or self._filter_sizes[i] != self._filter_sizes[i + 1]:
                    self._group_has_1x1[i] = True
                    self._num_non_ident_skips += 1

        self._num_main_conv_layers = 1 + sum(self._num_blocks) * 2

        self._has_bias = use_bias if use_bias == use_fc_bias else False
        self._has_fc_out = True
        self._mask_fc_out = True
        self._has_linear_out = True

        self._param_shapes = []
        self._param_shapes_meta = []
        self._internal_params = None
        self._hyper_shapes_learned = []
        self._hyper_shapes_learned_ref = []
        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()

        # Context modulation layers
        self._context_mod_layers = [] if self._use_context_mod else None
        if self._use_context_mod:
            cm_layer_inds = []
            cm_shapes = []
            if self._context_mod_inputs:
                cm_shapes.append([in_shape[2], *in_shape[:2]])
                cm_layer_inds.append(0)
            layer_out_shapes = self._compute_layer_out_sizes()
            cm_shapes.extend(layer_out_shapes)
            cm_layer_inds.extend(range(3, 3 * len(layer_out_shapes) + 1, 3))
            if self._no_last_layer_context_mod:
                cm_shapes = cm_shapes[:-1]
                cm_layer_inds = cm_layer_inds[:-1]
            if not self._context_mod_apply_pixel_wise:
                for i, s in enumerate(cm_shapes):
                    if len(s) == 3:
                        cm_shapes[i] = [s[0], 1, 1]
            self._add_context_mod_layers(cm_shapes, cm_layers=cm_layer_inds)

        # Batchnorm layers
        if use_batch_norm:
            bn_sizes = [self._filter_sizes[0]]
            for i in range(4):
                for _ in range(self._num_blocks[i]):
                    bn_sizes.extend([self._filter_sizes[i + 1]] * 2)
            bn_layers = list(range(2, 3 * len(bn_sizes) + 1, 3))
            if projection_shortcut:
                bn_layer_ind_skip = 3 * (self._num_main_conv_layers + 1) + 2
                for i in range(4):
                    if self._group_has_1x1[i]:
                        bn_sizes.append(self._filter_sizes[i + 1])
                        bn_layers.append(bn_layer_ind_skip)
                        bn_layer_ind_skip += 3
            self._add_batchnorm_layers(
                bn_sizes,
                no_weights,
                bn_layers=bn_layers,
                distill_bn_stats=distill_bn_stats,
                bn_track_stats=bn_track_stats,
            )

        # Initialize interval layers
        self.initial_conv = IntervalConv2d(
            in_channels=in_shape[2],
            out_channels=self._filter_sizes[0],
            kernel_size=self._init_kernel_size,
            stride=self._init_stride,
            padding=self._init_padding,
            device=device
        )
        self.initial_pool = IntervalAvgPool2d(kernel_size=2, stride=2)

        self.module_convs = []
        prev_fs = in_shape[2]
        for i in range(4):
            curr_fs = self._filter_sizes[i + 1]
            for j in range(self._num_blocks[i]):
                stride = 2 if j == 0 and i > 0 else 1
                self.module_convs.append(
                    IntervalConv2d(
                        in_channels=prev_fs if j == 0 else curr_fs,
                        out_channels=curr_fs,
                        kernel_size=(3, 3),
                        stride=stride,
                        padding=1,
                        device=device
                    )
                )
                self.module_convs.append(
                    IntervalConv2d(
                        in_channels=curr_fs,
                        out_channels=curr_fs,
                        kernel_size=(3, 3),
                        stride=1,
                        padding=1,
                        device=device
                    )
                )
                prev_fs = curr_fs

        self.skip_convs = []
        if projection_shortcut:
            n_in = self._filter_sizes[0]
            for i in range(4):
                if not self._group_has_1x1[i]:
                    self.skip_convs.append(None)
                    continue
                n_out = self._filter_sizes[i + 1]
                stride = 2 if i > 0 else 1
                self.skip_convs.append(
                    IntervalConv2d(
                        in_channels=n_in,
                        out_channels=n_out,
                        kernel_size=(1, 1),
                        stride=stride,
                        padding=0,
                        device=device
                    )
                )
                n_in = n_out

        self.interval_bn_layers = [IntervalBatchNorm() for _ in range(len(bn_sizes))]
        self.relu = IntervalReLU()
        self.final_pool = IntervalAvgPool2d(kernel_size=2, stride=2)
        self.flatten = IntervalFlatten()
        fc_in_size = self._filter_sizes[-1]  # Force 128 to match architecture
        self.linear = IntervalLinear(fc_in_size, num_classes, device=device)

        # Parameter shapes for hypernetwork
        if projection_shortcut:
            layer_ind_skip = 3 * (self._num_main_conv_layers + 1) + 1
            n_in = self._filter_sizes[0]
            for i in range(4):
                if not self._group_has_1x1[i]:
                    continue
                n_out = self._filter_sizes[i + 1]
                skip_1x1_shape = [n_out, n_in, 1, 1]
                self._hyper_shapes_learned.append(skip_1x1_shape)
                self._hyper_shapes_learned_ref.append(len(self._param_shapes))
                self._param_shapes.append(skip_1x1_shape)
                self._param_shapes_meta.append(
                    {"name": "weight", "index": -1, "layer": layer_ind_skip}
                )
                layer_ind_skip += 3
                n_in = n_out

        layer_id = 1
        prev_fs = self._in_shape[2]
        for i in range(5):
            if i == 0:
                num = 1
                curr_fs = self._filter_sizes[0]
                kernel_size = self._init_kernel_size
                stride = self._init_stride
                padding = self._init_padding
            else:
                num = self._num_blocks[i - 1] * 2
                curr_fs = self._filter_sizes[i]
                kernel_size = (3, 3)
                stride = 1
                padding = 1

            for n in range(num):
                if i > 0 and n % 2 == 0:
                    stride = 2 if i > 0 else 1
                else:
                    stride = 1
                layer_shapes = [[curr_fs, prev_fs, *kernel_size]]
                if use_bias:
                    layer_shapes.append([curr_fs])
                for s in layer_shapes:
                    self._hyper_shapes_learned.append(s)
                    self._hyper_shapes_learned_ref.append(len(self._param_shapes))
                    self._param_shapes.append(s)
                    self._param_shapes_meta.append(
                        {
                            "name": "weight" if len(s) != 1 else "bias",
                            "index": -1,
                            "layer": layer_id,
                        }
                    )
                layer_id += 3
                prev_fs = curr_fs

        layer_shapes = [[num_classes, fc_in_size]]
        if use_fc_bias:
            layer_shapes.append([num_classes])
        for s in layer_shapes:
            self._hyper_shapes_learned.append(s)
            self._hyper_shapes_learned_ref.append(len(self._param_shapes))
            self._param_shapes.append(s)
            self._param_shapes_meta.append(
                {
                    "name": "weight" if len(s) != 1 else "bias",
                    "index": -1,
                    "layer": layer_id,
                }
            )

        if verbose:
            print(
                f'Creating an interval "{self}" with {len(self._hyper_shapes_learned)} weight tensors'
                + (" with batchnorm." if use_batch_norm else ".")
            )
        self._is_properly_setup(check_has_bias=False)

    def __str__(self):
        return "IntervalResNet18-SplitMiniImageNet-Reduced"

    @property
    def has_bias(self):
        if self._use_bias != self._use_fc_bias:
            raise RuntimeError("has_bias does not apply when use_bias and use_fc_bias differ.")
        if self._use_bias and self._projection_shortcut:
            warn("Skip connections use 1x1 convs without biases. has_bias ignores these.")
        return self._has_bias

    def forward(self, x: torch.Tensor, epsilon: torch.Tensor, weights: list,
                distilled_params=None, condition=None, device="cuda") -> Tuple[torch.Tensor, torch.Tensor]:
        if (
            (not self._use_context_mod and self._no_weights)
            or (self._no_weights or self._context_mod_no_weights)
        ) and weights is None:
            raise Exception("Weights must be provided for no_weights networks.")

        assert weights is not None, "Weights must not be None"

        n_cm = self._num_context_mod_shapes()
        int_weights, cm_weights = None, None
        if isinstance(weights, dict):
            int_weights = weights.get("internal_weights")
            cm_weights = weights.get("mod_weights")
        else:
            cm_weights = weights[:n_cm] if self._use_context_mod else None
            int_weights = weights[n_cm:] if self._use_context_mod else weights
        if cm_weights is None and self._use_context_mod:
            if self._context_mod_no_weights:
                raise Exception("Context-mod weights required.")
            cm_weights = []  # Fallback
        if int_weights is None:
            if self._no_weights:
                raise Exception("Internal weights required.")
            int_weights = []  # Fallback
        if self._use_context_mod:
            assert len(cm_weights) == n_cm
        int_shapes = self.param_shapes[n_cm:]
        assert len(int_weights) == len(int_shapes), f"Expected {len(int_shapes)} weights, got {len(int_weights)}"
        for i, s in enumerate(int_shapes):
            assert list(int_weights[i].shape) == s, f"Shape mismatch at index {i}: expected {s}, got {int_weights[i].shape}"

        cm_weights_layer = []
        if cm_weights is not None:
            cm_start = 0
            for cm_layer in self.context_mod_layers:
                cm_end = cm_start + len(cm_layer.param_shapes)
                cm_weights_layer.append(cm_weights[cm_start:cm_end])
                cm_start = cm_end

        int_meta = self.param_shapes_meta[n_cm:]
        int_weights = list(int_weights)

        if self._use_batch_norm:
            lbw = 2 * len(self.batchnorm_layers)
            bn_weights = int_weights[:lbw]
            int_weights = int_weights[lbw:]
            bn_meta = int_meta[:lbw]
            int_meta = int_meta[lbw:]
            bn_scales = [bn_weights[2 * i] for i in range(len(self.batchnorm_layers))]
            bn_shifts = [bn_weights[2 * i + 1] for i in range(len(self.batchnorm_layers))]
        else:
            bn_scales, bn_shifts = [], []

        n_skip_1x1 = sum(self._group_has_1x1)
        skip_1x1_weights = [None] * 4
        for i in range(4):
            if self._group_has_1x1[i]:
                skip_1x1_weights[i] = int_weights.pop(0)
        int_meta = int_meta[n_skip_1x1:]

        layer_weights = [None] * (self._num_main_conv_layers + 1)
        layer_biases = [None] * (self._num_main_conv_layers + 1)
        for i, meta in enumerate(int_meta):
            ltype = meta["name"]
            lid = (meta["layer"] - 1) // 3
            if ltype == "weight":
                layer_weights[lid] = int_weights[i]
            else:
                layer_biases[lid] = int_weights[i]

        bn_cond = None
        cmod_cond = None
        if condition is not None:
            if isinstance(condition, dict):
                bn_cond = condition.get("bn_stats_id")
                cmod_cond = condition.get("cmod_ckpt_id")
            else:
                bn_cond = condition
        if cmod_cond is not None:
            raise NotImplementedError("Context-mod conditions not supported.")

        if self._use_batch_norm:
            nn = len(self.batchnorm_layers)
            running_means = [None] * nn
            running_vars = [None] * nn
            if distilled_params is not None:
                if not self._distill_bn_stats:
                    raise ValueError("distilled_params requires distill_bn_stats.")
                for i in range(0, len(distilled_params), 2):
                    running_means[i // 2] = distilled_params[i]
                    running_vars[i // 2] = distilled_params[i + 1]
            elif self._bn_track_stats and bn_cond is None:
                for i, bn_layer in enumerate(self.batchnorm_layers):
                    running_means[i], running_vars[i] = bn_layer.get_stats()
        else:
            running_means, running_vars = [], []

        cm_ind = 0
        bn_ind = 0
        layer_ind = 0
        conv_ind = 0

        def conv_layer(mu, eps, stride=1, padding=1, shortcut_mu=None, shortcut_eps=None,
                       no_conv=False, conv_layer=None):
            nonlocal layer_ind, cm_ind, bn_ind, conv_ind
            if not no_conv:
                mu, eps = conv_layer.forward(
                    mu, eps,
                    weight=layer_weights[layer_ind],
                    bias=layer_biases[layer_ind],
                    device=device
                )
                layer_ind += 1
            if self._use_context_mod and not self._context_mod_post_activation:
                mu = self._context_mod_layers[cm_ind].forward(
                    mu, weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond
                )
                cm_ind += 1
            if self._use_batch_norm:
                mu, eps = self.interval_bn_layers[bn_ind].forward(
                    mu, eps,
                    weight=bn_scales[bn_ind],
                    bias=bn_shifts[bn_ind],
                    running_mean=running_means[bn_ind],
                    running_var=running_vars[bn_ind],
                    stats_id=bn_cond,
                    batch_norm_forward=self.batchnorm_layers[bn_ind].forward,
                    device=device
                )
                bn_ind += 1
            if shortcut_mu is not None and shortcut_eps is not None:
                mu = mu + shortcut_mu
                eps = eps + shortcut_eps
            mu, eps = self.relu.forward(mu, eps, device=device)
            if self._use_context_mod and self._context_mod_post_activation:
                mu = self._context_mod_layers[cm_ind].forward(
                    mu, weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond
                )
                cm_ind += 1
            return mu, eps

        # Handle input format
        if not self._chw_input_format:
            x = x.view(-1, *self._in_shape)
            x = x.permute(0, 3, 1, 2)
            epsilon = epsilon * torch.ones_like(x)
        mu, eps = x.to(device), epsilon.to(device)

        if self._use_context_mod and self._context_mod_inputs:
            mu = self._context_mod_layers[cm_ind].forward(
                mu, weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond
            )
            cm_ind += 1

        # Initial conv and pool
        mu, eps = conv_layer(mu, eps, conv_layer=self.initial_conv)
        mu, eps = self.initial_pool.forward(mu, eps, device=device)

        # Modules
        fs_prev = self._filter_sizes[0]
        for i in range(4):
            fs_curr = self._filter_sizes[i + 1]
            for j in range(self._num_blocks[i]):
                shortcut_mu, shortcut_eps = mu, eps
                stride = 2 if j == 0 and i > 0 else 1
                if j == 0 and (stride != 1 or fs_prev != fs_curr):
                    if self._projection_shortcut and self._group_has_1x1[i]:
                        shortcut_mu, shortcut_eps = self.skip_convs[i].forward(
                            mu, eps,
                            weight=skip_1x1_weights[i],
                            bias=None,
                            device=device
                        )
                        if self._use_batch_norm:
                            bn_short = len(self.batchnorm_layers) - sum(self._group_has_1x1) + i
                            shortcut_mu, shortcut_eps = self.interval_bn_layers[bn_short].forward(
                                shortcut_mu, shortcut_eps,
                                weight=bn_scales[bn_short],
                                bias=bn_shifts[bn_short],
                                running_mean=running_means[bn_short],
                                running_var=running_vars[bn_short],
                                stats_id=bn_cond,
                                batch_norm_forward=self.batchnorm_layers[bn_short].forward,
                                device=device
                            )
                    else:
                        pad_left = (fs_curr - fs_prev) // 2
                        pad_right = (fs_curr - fs_prev + 1) // 2
                        if stride == 2:
                            shortcut_mu = shortcut_mu[:, :, ::2, ::2]
                            shortcut_eps = shortcut_eps[:, :, ::2, ::2]
                        shortcut_mu = F.pad(
                            shortcut_mu, (0, 0, 0, 0, pad_left, pad_right), "constant", 0
                        )
                        shortcut_eps = F.pad(
                            shortcut_eps, (0, 0, 0, 0, pad_left, pad_right), "constant", 0
                        )
                # First conv in block
                mu, eps = conv_layer(
                    mu, eps,
                    stride=stride,
                    padding=1,
                    conv_layer=self.module_convs[conv_ind]
                )
                conv_ind += 1
                # Second conv in block
                mu, eps = conv_layer(
                    mu, eps,
                    stride=1,
                    padding=1,
                    shortcut_mu=shortcut_mu,
                    shortcut_eps=shortcut_eps,
                    conv_layer=self.module_convs[conv_ind]
                )
                conv_ind += 1
            fs_prev = fs_curr

        # Final layers
        mu, eps = self.final_pool.forward(mu, eps, device=device)
        mu, eps = self.flatten.forward(mu, eps, device=device)
        mu, eps = self.linear.forward(
            mu, eps,
            weight=layer_weights[layer_ind],
            bias=layer_biases[layer_ind],
            device=device
        )

        if self._use_context_mod and not self._no_last_layer_context_mod:
            mu = self._context_mod_layers[cm_ind].forward(
                mu, weights=cm_weights_layer[cm_ind], ckpt_id=cmod_cond
            )

        return mu, eps

    def distillation_targets(self):
        if self.hyper_shapes_distilled is None:
            return None
        ret = []
        for bn_layer in self._batchnorm_layers:
            ret.extend(bn_layer.get_stats())
        return ret

    def _compute_layer_out_sizes(self):
        in_shape = [self._in_shape[2], *self._in_shape[:2]]
        fs = self._filter_sizes
        ret = []
        C, H, W = in_shape

        C = fs[0]
        H = (H - 3 + 2 * 1) // 2 + 1
        W = (W - 3 + 2 * 1) // 2 + 1
        H = H // 2
        W = W // 2
        ret.append([C, H, W])

        def add_block(H, W, C, stride):
            H = (H - 3 + 2 * 1) // stride + 1
            W = (W - 3 + 2 * 1) // stride + 1
            ret.append([C, H, W])
            H = (H - 3 + 2 * 1) // 1 + 1
            W = (W - 3 + 2 * 1) // 1 + 1
            ret.append([C, H, W])
            return H, W, C

        for i in range(4):
            for b in range(self._num_blocks[i]):
                stride = 2 if b == 0 and i > 0 else 1
                H, W, C = add_block(H, W, fs[i + 1], stride)

        H = H // 2
        W = W // 2
        ret.append([fs[-1], H, W])
        ret.append([self._num_classes])
        return ret

    def get_output_weight_mask(self, out_inds=None, device=None):
        return super().get_output_weight_mask(out_inds=out_inds, device=device)

if __name__ == "__main__":
    # Test the network
    net = IntervalResNet18(verbose=True, mode="default", num_classes=5)
    print(net)

    # Dummy input and weights
    batch_size = 16
    x = torch.randn(batch_size, 64, 64, 3)
    epsilon = torch.ones(batch_size, 64, 64, 3) * 0.1
    weights = [
        torch.randn(16, 3, 3, 3),  # Initial conv
        *[torch.randn(16, 16, 3, 3) for _ in range(8)],  # Module 1
        torch.randn(32, 16, 3, 3), *[torch.randn(32, 32, 3, 3) for _ in range(7)],  # Module 2
        torch.randn(64, 32, 3, 3), *[torch.randn(64, 64, 3, 3) for _ in range(7)],  # Module 3
        torch.randn(128, 64, 3, 3), *[torch.randn(128, 128, 3, 3) for _ in range(7)],  # Module 4
        torch.randn(32, 16, 1, 1), torch.randn(64, 32, 1, 1), torch.randn(128, 64, 1, 1),  # Skips
        *[torch.randn(s) for s in [16, 16, 16, 16, 32, 32, 32, 32, 64, 64, 64, 64, 128, 128, 128, 128, 32, 64, 128]],  # BN scales
        *[torch.randn(s) for s in [16, 16, 16, 16, 32, 32, 32, 32, 64, 64, 64, 64, 128, 128, 128, 128, 32, 64, 128]],  # BN shifts
        torch.randn(5, 128), torch.randn(5)  # Linear weight, bias
    ]

    mu, eps = net.forward(x, epsilon, weights, device="cpu")
    print(f"Output mu shape: {mu.shape}, eps shape: {eps.shape}")