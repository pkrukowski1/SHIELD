"""The implementation is based on https://hypnettorch.readthedocs.io/en/latest/_modules/hypnettorch/mnets/mlp.html#MLP"""

import torch
import torch.nn as nn
import timm

from hypnettorch.mnets.mnet_interface import MainNetInterface
from hypnettorch.utils.torch_utils import init_params

from typing import Tuple, List, Callable

class IntervalViT(nn.Module, MainNetInterface):
    """
    ViT is a neural network module that combines a Vision Transformer (ViT) feature extractor with a configurable multi-layer perceptron (MLP) head, designed for interval bound propagation and integration with hypernetworks.

    This class uses a pretrained ViT model from timm as a feature extractor and appends an MLP head whose weights are expected to be provided externally (e.g., by a hypernetwork). The MLP head supports configurable hidden layers, activation functions, and optional bias terms. The forward method propagates both the input and an interval epsilon through the network, enabling robustness analysis via interval bound propagation.

    Inheritance:
        nn.Module: PyTorch base class for all neural network modules.
        MainNetInterface: Interface for main network behavior in hypnettorch.

    Args:
        n_out (int): Number of output features for the MLP head. Default is 1.
        hidden_layers (tuple): Sizes of hidden layers in the MLP head. Default is (10, 10).
        activation_fn (callable): Activation function to use between MLP layers. Default is torch.nn.ReLU().
        use_bias (bool): Whether to include bias terms in MLP layers. Default is True.
        no_weights (bool): If True, the network does not create learnable parameters (weights must be provided externally). Must be True.
        out_fn (callable, optional): Optional output transformation function. Default is None.

    Attributes:
        feature_extractor (nn.Module): Pretrained ViT model for feature extraction.
        _a_fun (callable): Activation function for the MLP head.
        _no_weights (bool): Indicates if weights are not learnable.
        _out_fn (callable): Output transformation function.
        _has_bias (bool): Indicates if bias is used in the MLP head.
        _has_fc_out (bool): Indicates if the network has a fully connected output layer.
        _mask_fc_out (bool): Indicates if the output layer is masked.
        _has_linear_out (bool): Indicates if the output layer is linear.
        _param_shapes (list): List of parameter shapes for each MLP layer.
        _param_shapes_meta (list): Metadata for each parameter shape.
        _weights (nn.ParameterList): List of network parameters (weights and biases).
        _layer_weight_tensors (nn.ParameterList): List of weight tensors for each MLP layer.
        _layer_bias_vectors (nn.ParameterList): List of bias vectors for each MLP layer.

    Methods:
        forward(x, epsilon, weights=None, distilled_params=None, condition=None):
            Performs a forward pass with interval bound propagation, returning the output and propagated epsilon.
        distillation_targets():
            Returns distillation targets (None for this implementation).
        weight_shapes(n_in=1, n_out=1, hidden_layers=[10, 10], use_bias=True):
            Static method to compute the shapes of weights and biases for each MLP layer.

    Notes:
        - The MLP head is designed to work with externally provided weights (e.g., from a hypernetwork).
        - The forward method propagates both the input and an interval epsilon through the MLP head, supporting interval bound propagation for robustness analysis.
        - Biases are handled according to the use_bias flag.
        - The network asserts that no_weights is True, enforcing the use of external weights.
    """
    
    def __init__(
        self,
        n_out: int=1,
        hidden_layers: Tuple=(10, 10),
        activation_fn: nn.Module=torch.nn.ReLU(),
        use_bias: bool=True,
        no_weights: bool=True,
        out_fn: Callable=None,
    ) -> None:
        # FIXME find a way using super to handle multiple inheritance.
        nn.Module.__init__(self)
        MainNetInterface.__init__(self)

        assert no_weights is True, "Learnable parameters are only generated by a hypernetwork"

        self.feature_extractor = timm.create_model(model_name="vit_base_patch16_224", pretrained=True)
        n_in = self.feature_extractor.head.in_features

        hidden_layers = list(hidden_layers)

        self._a_fun = activation_fn
        self._no_weights = no_weights
        self._out_fn = out_fn

        self._has_bias = use_bias
        self._has_fc_out = True
        self._mask_fc_out = True
        self._has_linear_out = True

        self._param_shapes = []
        self._param_shapes_meta = []
        self._weights = (
            nn.ParameterList()
        )

        ### Compute shapes of linear layers.
        linear_shapes = IntervalViT.weight_shapes(
            n_in=n_in,
            n_out=n_out,
            hidden_layers=hidden_layers,
            use_bias=use_bias,
        )
        self._param_shapes.extend(linear_shapes)

        for i, s in enumerate(linear_shapes):
            self._param_shapes_meta.append(
                {
                    "name": "weight" if len(s) != 1 else "bias",
                    "index": -1 if no_weights else len(self._weights) + i,
                    "layer": -1,  # 'layer' is set later.
                }
            )

        layer_ind = 0
        for i, dd in enumerate(self._param_shapes_meta):
            dd["layer"] = 1 + layer_ind
            if not use_bias or dd["name"] == "bias":
                layer_ind += 1

        self._layer_weight_tensors = nn.ParameterList()
        self._layer_bias_vectors = nn.ParameterList()
        for i, dims in enumerate(linear_shapes):
            self._weights.append(
                nn.Parameter(torch.Tensor(*dims), requires_grad=True)
            )
            if len(dims) == 1:
                self._layer_bias_vectors.append(self._weights[-1])
            else:
                self._layer_weight_tensors.append(self._weights[-1])

        for i in range(len(self._layer_weight_tensors)):
            if use_bias:
                init_params(
                    self._layer_weight_tensors[i],
                    self._layer_bias_vectors[i],
                )
            else:
                init_params(self._layer_weight_tensors[i])

        self._is_properly_setup()

    def forward(self, x: torch.Tensor, epsilon: float, weights: List[torch.Tensor]=None, 
                distilled_params=None, condition: int=None) -> Tuple[torch.Tensor,torch.Tensor]:
        """
        Perform a forward pass with interval bound propagation.

        Args:
            x (torch.Tensor): Input tensor.
            epsilon (float): Radii of a hypercube around x.
            weights (list of torch.Tensor, optional): List of weight and bias tensors. If None, uses self.weights.
            distilled_params: Unused. For interface compatibility.
            condition: Unused. For interface compatibility.

        Returns:
            tuple: (mu, eps)
            mu (torch.Tensor): Output midpoint after propagation.
            eps (torch.Tensor): Output radii after propagation.
        """
        if weights is None:
           weights = self.weights

        w_weights = []
        b_weights = []

        for i, p in enumerate(weights):
            if self.has_bias and i % 2 == 1:
                b_weights.append(p)
            else:
                w_weights.append(p)

        x = x.view(-1, 3, 224, 224)
        
        # Forward through ViT
        tokens = self.feature_extractor.forward_features(x)  # shape: (B, 197, 768)
        cls_token = tokens[:, 0]                             # shape: (B, 768)
        hidden = cls_token

        # Initialize eps of the same shape
        eps = epsilon * torch.ones_like(hidden)  # shape: (B, 768)

        # Interval-based forward
        for l in range(len(w_weights)):
            W = w_weights[l]                     # shape: (D_out, D_in)
            b = b_weights[l] if self.has_bias else None

            hidden = hidden @ W.T               # shape: (B, D_out)
            eps = (torch.abs(W) @ eps.T).T      # shape: (B, D_out)

            if b is not None:
                hidden = hidden + b             # broadcast over batch

            if l < len(w_weights) - 1:
                z_lower = self._a_fun(hidden - eps)
                z_upper = self._a_fun(hidden + eps)
                hidden = (z_upper + z_lower) / 2
                eps = (z_upper - z_lower) / 2   # shape: (B, D_out)

        return hidden, eps

    def distillation_targets(self):
        return None

    @staticmethod
    def weight_shapes(n_in: int=1, n_out: int=1, hidden_layers: Tuple[int]=[10, 10], use_bias: bool=True):
        """
        Compute the shapes of weights and biases for each layer of the MLP based classifier.

        Args:
            n_in (int, optional): Number of input features. Defaults to 1.
            n_out (int, optional): Number of output features. Defaults to 1.
            hidden_layers (list or tuple of int, optional): Sizes of hidden layers. Defaults to [10, 10].
            use_bias (bool, optional): Whether to include bias shapes for each layer. Defaults to True.

        Returns:
            list: List of shapes for weights and (optionally) biases for each layer, in order.
              Each weight shape is [out_features, in_features], and each bias shape is [out_features].
        """
        shapes = []
        prev_dim = n_in
        layer_out_sizes = hidden_layers + [n_out]
        for i, size in enumerate(layer_out_sizes):
            shapes.append([size, prev_dim])
            if use_bias:
                shapes.append([size])
            prev_dim = size

        return shapes


if __name__ == "__main__":
    pass
