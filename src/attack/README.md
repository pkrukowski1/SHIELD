## `attack`  

This directory contains adversarial attack implementations used in the SHIELD project:

- `auto_attack.py` – Implementation of AutoAttack, a parameter-free and robust evaluation framework.
- `fgsm.py` – Implementation of the Fast Gradient Sign Method (FGSM) for generating adversarial examples.
- `pgd.py` – Implementation of Projected Gradient Descent (PGD), a strong iterative attack used in adversarial robustness evaluation.
